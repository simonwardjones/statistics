{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book is about learning from data. \n",
    "\n",
    "In a typical **supervised** scenario, we have outcomes (dependent/output/response variable), usually quantitative/categorical that we wish to predict based on a set of features (independent variables/inputs/predictors). We have a training set of data, in which we observe the outcome and feature. We build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. \n",
    "\n",
    "In the **unsupervised** learning problem, we observe only the features and have no measurements of the outcome. Our task is to describe how the data are organized or clustered.\n",
    "\n",
    "Examples:\n",
    " - Email spam classification\n",
    " - Prostate cancer regression prediction of log prostate specific antigen\n",
    " - Handwritten digit recognition\n",
    " - DNA unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning we use the term `regression` when the output is quantitative and `classification` when the output is qualitative (a.k.a categorical/discrete). A third variable type is ordered categorical, such as small, medium and large.\n",
    "\n",
    "Given X, make a good prediction of the output Y, denoted by $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the relationship between X and Y as a linear function of X. We include a 1 in the X vector to account for the intercept. (here X is a single vector, but in general it can be a matrix where each row is a sample and each column is a feature, in this case Y would be a vector of outcomes)\n",
    "$$\\hat{Y}  = \\hat{\\beta}_0 + \\sum_{j=1}^pX_j\\hat{\\beta}_j\\\\ = X^{T}\\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model the most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n",
    "Minimising this gives the least squares estimates of the coefficients \n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest-Neighbor Methods\n",
    "We use the k-nearest neighbors to predict the outcome of a new sample. \n",
    "$$\\hat{Y}(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)}y_i$$\n",
    "where $N_k(x)$ is the set of k points in the training set closest to x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Theory\n",
    "\n",
    "In general we seek function $f(X)$ predicting Y. We define the loss function $L(Y, f(X))$ which measures the cost of predicting $f(X)$ when the true value is Y. The most common loss function is the squared error loss \n",
    "$$L(Y, f(X)) = (Y - f(X))^2$$\n",
    "The value of $f(X)$ that minimizes the expected prediction error is the conditional expectation of Y given X, $f(x) = E(Y|X=x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of dimensionality\n",
    "\n",
    "Looking at the nearest neighbours method, as the number of dimensions increases, the volume of the space increases so that the nearest neighbours are no longer as close. This is the curse of dimensionality and increases the bias of the nearest neighbour method.\n",
    "\n",
    "The book shows that by leveraging structure of the problem, such as linearity, we can reduce the impact of the curse of dimensionality reducing the bias and variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Supervised Learning and Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Often we can reframe the supervised problem as a statistical model where\n",
    "$$\n",
    "Y = f(X) + \\epsilon\n",
    "$$\n",
    "where $E(\\epsilon) = 0$\n",
    "\n",
    "We want to estimate $f(X)$ using the training data. We can view this as a `learning problem` where we iteratively improve our estimate of $f(X)$ or as a `function approximation problem`.\n",
    "\n",
    "To find the optimal function we often minimize the square loss but a more general approach is to maximised the likelihood of the data given some assumed model. In the case of a linear model assuming the errors are normally distributed we can show maximising the likelihood is equivalent to minimizing the square loss.\n",
    "\n",
    "Besides a linear model or nearest neighbours other methods include:\n",
    " - `Roughness penalty methods`: These reduce model complexity by penalizing the complexity of the model in the loss function. This is also known as `regularization`.\n",
    " - `Kernel methods`, similar to nearest neighbours but with a `weighted average of the neighbours` to account for the distance.\n",
    " - `Basis functions` assume f is of the form $\\sum_{j=1}^M\\theta_jh_j(x)$ where $h_j(x)$ are functions of x. These approaches linear models, splines, single layer neural networks and radial basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bias variance tradeoff\n",
    "\n",
    "Many models have a smoothing parameter that controls the complexity of the model. The choice of this parameter is a tradeoff between bias and variance. A model with high bias will underfit the data, while a model with high variance will overfit the data.\n",
    "\n",
    "The expected prediction error can be decomposed into the irreducible error, the squared bias and the variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Chapter 3 - Linear methods for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assume $E(Y|X)$ is linear in $X_i$. We have an input vector $X^T = (X_1, X_2, ..., X_p)$ and we want to predict a real-valued output Y.\n",
    "$$\n",
    "    f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j\n",
    "$$\n",
    "We have a training set of N observations $(x_1, y_1), ..., (x_N, y_N)$ and we want to estimate the coefficients $\\beta_0, \\beta_1, ..., \\beta_p$. Each $x_i$ is a p-dimensional vector $x_i = (x_{i1}, x_{i2}, ..., x_{ip})^T$.\n",
    "The most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this we can differentiate the RSS with respect to $\\beta$ and set to zero\n",
    "$$\n",
    "    \\frac{\\partial RSS}{\\partial \\beta} = -2X^T(y - X\\beta) = 0\n",
    "$$\n",
    "This gives the normal equations\n",
    "$$\n",
    "    X^TX\\beta = X^Ty\n",
    "$$\n",
    "and the least squares estimate of $\\beta$ is\n",
    "$$\n",
    "    \\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values for the training data are\n",
    "$$\n",
    "    \\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "We denote the matrix $H = X(X^TX)^{-1}X^T$ as the `hat matrix` because it puts a `hat` on y. Geometrically this matrix projects y orthogonally onto the space spanned by the columns of X.\n",
    "\n",
    "$$\n",
    "    var(\\hat{\\beta}) = var((X^TX)^{-1}X^Ty) = (X^TX)^{-1}X^Tvar(y)((X^TX)^{-1}X^T)^T = \\\\\n",
    "     = (X^TX)^{-1}X^Tvar(y)X(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}\n",
    "$$\n",
    "\n",
    "Typically we don't know $\\sigma^2$ so we estimate it with the residual sum of squares\n",
    "$$\n",
    "    \\hat{\\sigma}^2 = \\frac{1}{N-p-1}\\sum_{i=1}^N(y_i - x_i^T\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "Assuming the errors are normal and the model is correct then\n",
    "$$\n",
    "\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{(N-p-1)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{N-p-1}\n",
    "$$\n",
    "We can use this to construct confidence intervals for the coefficients and to test hypotheses about the coefficients. We can calculate the $z_j$ values which follow a t-distribution with N-p-1 degrees of freedom and allow us to test the null hypothesis $\\beta_j = 0$.\n",
    "$$\n",
    "z_j = \\frac{\\hat{\\beta}_j}{\\sqrt{\\hat{\\sigma}^2((X^TX)^{-1})_{jj}}}\n",
    "$$\n",
    "Note the standard error of $\\hat{\\beta}_j$ is $\\sqrt{\\hat{\\sigma}^2((X^TX)^{-1})_{jj}}$\n",
    "\n",
    "#### Gauss Markov Theorem\n",
    "The least squares estimates are the best linear unbiased estimates (BLUE) of the coefficients. This means they have the smallest variance of all linear unbiased estimates. A linear unbiased estimate is one that is unbiased and is a linear function of the data. By linear we mean of the form $c^Ty$ where c depends on the data but not on the parameter being estimated. Unbiased means $E(c^Ty) = c^TE(y) = c^TX\\beta$.\n",
    "\n",
    "The Gram-Schmidt procedure can be used to orthogonalize the columns of X. This can be useful when the columns are highly correlated. The multiple regression coefficient $\\beta_j$ represents the marginal effect of $X_j$ on Y after adjusting for all other variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Subset selection\n",
    "\n",
    " - *Best Subset selection* fits regressions for every subset of features finding the one with the minimum least squares error.\n",
    " - *Forward and stepwise selection* starts with just an intercept and adds features one at a time. Forward selection adds the feature that most reduces the RSS. Backward selection starts with all features and removes the one that least increases the RSS.\n",
    " - *Forward stagewise* regression starts like forward stepwise regression fitting just the intercept. At it step it finds the variable most correlated with the current residual. It then adds the simple linear regression coefficient of this variable to the current coefficient. This is repeated until no variable has a correlation with the residual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression\n",
    "\n",
    "Beta is chosen to minimize the penalized residual sum of squares\n",
    "\n",
    "$$\n",
    "    \\hat{\\beta}_{ridge} = \\underset{\\beta}{\\mathrm{argmin}} \\left\\{ \\sum_{i=1}^N(y_i - x_i^T\\beta)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 \\right\\}\n",
    "$$\n",
    "\n",
    "In vector notation\n",
    "$$\n",
    "    RSS(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\hat{\\beta}_{ridge} = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at SVD \n",
    "\n",
    "The singular value decomposition of X is \n",
    "$$\n",
    "    X = UDV^T\n",
    "$$\n",
    "Where U is an N * P orthogonal matrix, D is a P * P diagonal matrix where d_1 >= d_2 >= ... >= d_p >= 0 and V is a P * P orthogonal matrix.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X\\hat{\\beta}_{LS} &= X(X^TX)^{-1}X^Ty \\\\\n",
    "    &= UDV^T((UDV^T)^TUDV^T)^{-1}(UDV^T)^Ty \\\\\n",
    "    &= UDV^T(VDU^TUDV^T)^{-1}VDU^Ty \\\\\n",
    "    &= UDV^T(VD^2V^T)^{-1}VDU^Ty \\\\\n",
    "    &= UDV^TV(D^2)^{-1}V^TVDU^Ty \\\\\n",
    "    &= UD(D^2)^{-1}DU^Ty \\\\\n",
    "    &= UU^Ty\n",
    "\\end{align*}\n",
    "$$\n",
    "And the ridge regression solution is\n",
    "$$\n",
    "    X\\hat{\\beta}_{ridge} = UD(D^2 + \\lambda I)^{-1}DU^Ty \\\\\n",
    "    = \\sum_{j=1}^pu_j\\frac{d_j^2}{d_j^2 + \\lambda}u_j^Ty\n",
    "$$\n",
    "Where $u_j$ is the jth column of U and $d_j$ is the jth diagonal element of D.\n",
    "Note this shows the solution projects y onto the space spanned by the columns of U (so X) and shrinks the coefficients by a factor of $\\frac{d_j^2}{d_j^2 + \\lambda}$. Note the $u_j$ are the principal components of X and due to the factor the ridge regression shrinks the coefficients of the principal components with the smallest variance the most.\n",
    "\n",
    "**Relation to sample covariance**\n",
    "\n",
    "The sample covariance matrix of X is $S = X^TX/N$. Note \n",
    "$$\n",
    "    X^TX = (UDV^T)^TUDV^T = VDU^TUDV^T = VD^2V^T\n",
    "$$\n",
    "which is the eigen decomposition of $X^TX$. The eigenvectors of $X^TX$ are the columns of V and the eigenvalues are the squares of the singular values of X. $z_i = Xv_i$ are the principal components of X.\n",
    "$$\n",
    "   z_i = Xv_i = UDV^Tv_i = UD\\mathbb{1}_{j=i} = d_iu_i\n",
    "$$\n",
    "The first principal component is the direction in which the data varies the most. The second principal component is the direction orthogonal to the first in which the data varies the most and so on. The variance of the data in the direction of the ith principal component is $d_i^2/N$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression\n",
    "\n",
    "It is similar to ridge regression but uses the L1 norm instead of the L2 norm. This leads to sparsity in the coefficients. The lasso estimate is\n",
    "$$\n",
    "    \\hat{\\beta}_{lasso} = \\underset{\\beta}{\\mathrm{argmin}} \\left\\{ \\sum_{i=1}^N(y_i - x_i^T\\beta)^2 + \\lambda\\sum_{j=1}^p|\\beta_j| \\right\\}\n",
    "$$\n",
    "or in verctor notation\n",
    "$$\n",
    "    RSS(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda||\\beta||_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic net\n",
    "\n",
    "The elastic net is a combination of the lasso and ridge regression. It has two tuning parameters $\\alpha$ and $\\lambda$ and\n",
    "$$\n",
    "    RSS(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda\\left(\\alpha||\\beta||_1 + (1-\\alpha)||\\beta||_2^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least angle regression\n",
    "\n",
    "Least angle regression is a method for fitting a linear model with a large number of features. It is somewhat similar to forward stepwise regression. First the matrix X is normalised so that each predictor has mean 0 and variance 1.\n",
    "\n",
    "To start it sets all Beta coefficients to 0 and finds the variable $x_i$ most correlated with y, this makes up the current active set. For all variables in the active set it moves the coefficients towards the least squares solution for the current active set and the residual. When another variable becomes as correlated with the residual as one of the active variables it adds it to the active set.\n",
    "\n",
    "Lasso modification: If a non-zero coefficient hits zero then it is removed from the active set and the update direction is recalculated.\n",
    "\n",
    "#### Degrees of freedom\n",
    "\n",
    "A more general definition of the degrees of freedom can be made as\n",
    "$$\n",
    "    df(\\hat{y}) = \\frac{1}{\\sigma^2}\\sum_{i=1}^Ncov(\\hat{y}_i, y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Principle Components regression\n",
    "\n",
    "Principle component regression regresses Y on the first M principle components of X, $z_1, z_2, ..., z_m$ . This regression leads to projections\n",
    "$$\n",
    "    \\hat{y}^{PCR}_{(M)} = \\bar{y}\\mathbb{1} + \\sum_{m=1}^M\\hat{\\theta}_mz_m\n",
    "$$\n",
    "The coefficients $\\hat{\\theta}_m$ are found by regressing y on the principle components and as they are orthogonal $\\theta_m = \\frac{<z_m, y>}{<z_m, z_m>} = \\frac{z_m^Ty}{z_m^Tz_m}$\n",
    "\n",
    "We see that principal components regression is very similar to ridge regression: both operate via the principal components of the input matrix. Ridge regression shrinks the coefficients of the principal components, shrinking more depending on the size of the corresponding eigenvalue; principal components regression discards the p âˆ’ M smallest eigenvalue components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Partial least squares\n",
    "\n",
    "First we assume X is normalised so that each predictor has mean 0 and variance 1. For m in 1, ...., p we create the mth partial least squares direction $z_m = \\sum_{j=1}^p\\phi_{mj}x_j^{(m-1)}$ where $\\phi_{mj} = <x_j, y>$ and $x_j^{(0)} = x_j$. We then orthogonalize each $x_j^{(m)} w.r.t $z_m$ as $x_j^{(m)} = x_j^{(m-1)} - \\frac{<x_j^{(m-1)}, z_m>}{<z_m, z_m>}z_m$. We then regress y on $z_1, z_2, ..., z_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, PLS, PCR and ridge regression tend to behave similarly.\n",
    "Ridge regression may be preferred because it shrinks smoothly, rather than\n",
    "in discrete steps. Lasso falls somewhere between ridge regression and best\n",
    "subset regression, and enjoys some of the properties of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 4 - Linear methods for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminant analysis\n",
    "\n",
    "This is a class of methods that model discriminant functions $\\delta_k(x)$ for each class, and then classify x\n",
    "to the class with the largest value for its discriminant function. The decision boundary is the set of points where the discriminant functions are equal.\n",
    "\n",
    "#### Linear Regression of class indicators\n",
    "\n",
    "A simple example of discriminant analysis is to fit a linear regression model for the vector of class indicators. If we have k classes we define $y_i$ as a vector of length k with a 1 in the ith position and 0 elsewhere. We then fit a linear regression model to predict $y_i$ from x. We stack the $y_i$ vectors to form a matrix Y and fit a linear regression model to predict Y from X. The projections are\n",
    "$$\n",
    "    \\hat{Y} = X\\hat{B} = X(X^TX)^{-1}X^TY\n",
    "$$\n",
    "The predicted class is the one with the largest value in the vector $\\hat{y}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "We wish to model the $P(G = k|X = x)$, the probability that a given observation X = x belongs to the kth class. We can use Bayes theorem to write this posterior probability as\n",
    "$$\n",
    "    P(G = k|X = x) = \\frac{P(X = x|G = k)P(G = k)}{P(X = x)}\n",
    "$$\n",
    "And if we let $\\pi_k = P(G = k)$ be the prior probability of class k and $f_k(x) = P(X = x|G = k)$ be the class conditional density of X in class k then\n",
    "$$\n",
    "    P(G = k|X = x) = \\frac{\\pi_kf_k(x)}{\\sum_{l=1}^K\\pi_lf_l(x)}\n",
    "$$\n",
    "\n",
    "Suppose that the class conditional densities $f_k(x)$ are multivariate normal with mean $\\mu_k$ and covariance $\\Sigma_k$. Then the density function is\n",
    "$$\n",
    "    f_k(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma_k|^{1/2}}e^{-\\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k)}\n",
    "$$\n",
    "\n",
    "In LDA we assume the lasses have a common covariance matrix $\\Sigma_k = \\Sigma$. If we look at the log ratio of the posterior probabilities for two classes we see that the decision boundary is linear in x. This is because the quadratic terms in the exponent cancel out. This is why it is called linear discriminant analysis.\n",
    "\n",
    "$$\n",
    "    log(\\frac{P(G = k|X = x)}{P(G = l|X = x)}) = log(\\frac{\\pi_k}{\\pi_l}) - \\frac{1}{2}(\\mu_k + \\mu_l)^T\\Sigma^{-1}(\\mu_k - \\mu_l) + x^T\\Sigma^{-1}(\\mu_k - \\mu_l)\n",
    "$$\n",
    "\n",
    "An equivalent description of the decision rule is to use the discriminant functions \n",
    "$$\n",
    "\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + log(\\pi_k)\n",
    "$$\n",
    "and classify x to the class with the largest discriminant function.\n",
    "\n",
    "in practise we don't know the parameters of the model so we estimate them from the training data. We estimate $\\pi_k$ by the proportion of class k in the training data. We estimate $\\mu_k$ by the sample mean of the training data in class k and we estimate $\\Sigma$ by the pooled sample covariance matrix $\\hat{\\Sigma} = \\sum_{k=1}^K\\sum_{g_i = k}(x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T/(N - K)$\n",
    "\n",
    "#### Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "If we don't assume a common covariance matrix then we have quadratic discriminant analysis. The decision boundary is now quadratic in x. The discriminant function is \n",
    "$$\n",
    "    \\delta_k(x) = -\\frac{1}{2}log|\\Sigma_k| - \\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k) + log(\\pi_k)\n",
    "$$\n",
    "\n",
    "##### Calculations\n",
    "\n",
    "As $\\Sigma$ and $\\Sigma_k$ is positive deinfinite we can write it as $\\Sigma_k = U_kD_kU_k^T$ where $U_k$ is an orthogonal matrix and $D_k$ is a diagonal matrix. We can then write the discriminant function as\n",
    "$$\n",
    "    \\delta_k(x) = -\\frac{1}{2}log|D_k| - \\frac{1}{2}(D_k^{-1/2}U_k^T(x - \\mu_k))^T(D_k^{-1/2}U_k^T(x - \\mu_k)) + log(\\pi_k)\n",
    "$$\n",
    "and we can see that the LDA can be calculated by sphering the data with $D^{-1/2}U^TX$ and then finding the closest class mean modulo $\\pi_k$ assuming a common covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Regularized Discriminant Analysis\n",
    "\n",
    "Friedman proposed a regularized version of LDA as a compromise between LDA and QDA. The regularized version shrinks the class covariance matrices towards a common covariance matrix. \n",
    "$$\n",
    "    \\hat{\\Sigma}_k = \\alpha\\hat{\\Sigma}_k + (1 - \\alpha)\\hat{\\Sigma}\n",
    "$$\n",
    "In practise $\\alpha$ is chosen by cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduced Rank Linear Discriminant Analysis\n",
    "\n",
    "We noticed above that the LDA only uses the distance to the class centroids and the common covariance matrix. The centroids themselves are in a subspace (of dimension K - 1) spanned by the class centroids. It is enough to project the data onto this subspace and make the decision based on the distance to the centroids in this subspace.\n",
    "\n",
    "Fisher proposes that the subspace is optimal when the class means of sphered data have maximum separation in this subspace in terms of variance. Following this definition, optimal subspace coordinates are simply found by doing PCA on sphered class means.\n",
    "\n",
    "We define M as a k * p matrix with the class centroids as rows and W the within class covariance matrix with dimensions p * p. We estimate W as $$W = \\sum_{k=1}^K\\sum_{g_i = k}(x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T$$.\n",
    "We then compute $M^* = MW^{(-1/2)}$ and $B* = Cov(M^*)$ by \n",
    "$$\n",
    "B* = \\sum_{k=1}^K(\\mu^*_k - \\hat{\\mu^*})(\\mu^*_k - \\hat{\\mu^*})^T$$\n",
    "Where $\\mu^*_k$ is the kth row of $M^*$ and $\\hat{\\mu^*}$ is the mean of the rows of $M^*$. We then compute the eigenvectors of $B*$, $v^*_1, v^*_2, ..., v^*_K$. The lth discriminant variable is then \n",
    "$$\n",
    "Z_l = v^T_lX = (W^{-1/2}v^*_l)^TX\n",
    "$$.\n",
    "Where $v_l$ are the discriminant coordinates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x whilst ensuring they sum to 1 and fall in the range 0 to 1. The model has the form\n",
    "$$\n",
    "    log\\left(\\frac{P(G = k|X = x)}{P(G = K|X = x)}\\right) = \\beta_{k0} + \\beta_k^Tx \\quad\\text{ for } k = 1, ..., K - 1\n",
    "$$\n",
    "Then we can write the posterior probabilities as\n",
    "$$\n",
    "    P(G = k|X = x) = \\frac{e^{\\beta_{k0} + \\beta_k^Tx}}{1 + \\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_l^Tx}}\\\\\n",
    "    P(G = K|X = x) = \\frac{1}{1 + \\sum_{l=1}^{K-1}e^{\\beta_{l0} + \\beta_l^Tx}}\n",
    "$$\n",
    "\n",
    "The likelihood function is\n",
    "$$\n",
    "    L(\\beta) = \\prod_{i=1}^N p_{g_i}(x_i)\n",
    "$$\n",
    "where $p_{g_i}(x_i)$ is the probability of the ith observation given x_i and beta. We can then maximize the log likelihood to find the coefficients.\n",
    "The log likelihood is\n",
    "$$\n",
    "    l(\\beta) = \\sum_{i=1}^Nlog(p_{g_i}(x_i))\n",
    "$$\n",
    "In the two class case the log likelihood is\n",
    "$$\n",
    "    l(\\beta) = \\sum_{i=1}^N\\left\\{y_ilog(p(x_i)) + (1 - y_i)log(1 - p(x_i))\\right\\}\n",
    "$$\n",
    "and given\n",
    "$$\n",
    "    p(x) = \\frac{e^{\\beta_0 + \\beta^Tx}}{1 + e^{\\beta_0 + \\beta^Tx}}\n",
    "$$\n",
    "$$\n",
    "    l(\\beta) = \\sum_{i=1}^N\\left\\{y_i(\\beta_0 + \\beta^Tx_i) - log(1 + e^{\\beta_0 + \\beta^Tx_i})\\right\\}\n",
    "$$\n",
    "To maximize the log likelihood we can set the derivative of the log likelihood with respect to $\\beta$ to zero and solve for $\\beta$.\n",
    "$$\n",
    "    \\frac{\\partial l(\\beta)}{\\partial \\beta} = \\sum_{i=1}^N\\left\\{y_i - \\frac{e^{\\beta_0 + \\beta^Tx_i}}{1 + e^{\\beta_0 + \\beta^Tx_i}}\\right\\}x_i = 0\n",
    "$$\n",
    "We solve this using newton's method or gradient descent.\n",
    "\n",
    "\n",
    "The maximum likelihood estimates of the coefficients $\\hat{\\beta}$ satisfy a special relationship - they are the coefficients of a weighted least squares regression of $$z_i = x_i^T\\hat{\\beta} + \\frac{(y_i - \\hat{p}_i)}{\\hat{p}_i(1 - \\hat{p}_i)}$$ on the predictors x_i where the weights are $\\hat{p}_i(1 - \\hat{p}_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### l1 Regularized logistic regression\n",
    "\n",
    "We can add a penalty term to the log likelihood to prevent overfitting. This is known as l1 regularized logistic regression. The penalty term is $\\lambda\\sum_{j=1}^p|\\beta_j|$. The log likelihood is then\n",
    "$$\n",
    "    l(\\beta) = \\sum_{i=1}^N\\left\\{y_i(\\beta_0 + \\beta^Tx_i) - log(1 + e^{\\beta_0 + \\beta^Tx_i})\\right\\} - \\lambda\\sum_{j=1}^p|\\beta_j|\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression vs LDA\n",
    "\n",
    "Both LDA and logistic regression have linear log-its. Logistic regression maximises the conditional likelihood of the data given the class labels $P(G|X)$, where as LDA maximises the joint likelihood of the data and the class labels $P(G, X)$ assuming the class conditional densities are normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating hyperplanes\n",
    "\n",
    "In both LDA and logistic regression the decision boundary is linear in x. This means the decision boundary is a hyperplane. The hyperplane is the set of points where the discriminant function is equal for two classes. \n",
    "We can instead aim to directly find a hyperplane that separates the classes.\n",
    "\n",
    "For an optimal separating hyperplane we wish to maximize the margin between the classes. The margin is the distance between the hyperplane and the closest point in the training data. The optimal hyperplane is the one that maximizes the margin or in mathematical notation\n",
    "$$\n",
    "    max_{\\beta, \\beta_0, ||\\beta|| = 1}M \\quad\\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq M \\quad\\text{for } i = 1, ..., N\n",
    "$$\n",
    "\n",
    "This is equivalent to\n",
    "$$\n",
    "    min_{\\beta, \\beta_0} \\frac{1}{2}||\\beta||^2 \\quad\\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\geq 1 \\quad\\text{for } i = 1, ..., N\n",
    "$$\n",
    "Solving this via the Lagrange multipliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
