{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5f2bd7-6b62-4a6b-8a00-469e4889a63a",
   "metadata": {},
   "source": [
    "# Deep Learning Specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a532ec-35f9-468d-8fa5-0df23e6717b5",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning, regulatrisation and optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e3ad3-362f-491f-854c-7575f70037d5",
   "metadata": {},
   "source": [
    "### L2 regularisation, dropout, vanishing and exploding gradients, stadard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec321c",
   "metadata": {},
   "source": [
    "### Bias variance\n",
    "\n",
    "![bias-variance](./img/bias-variance.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fddb8c",
   "metadata": {},
   "source": [
    "### Train, develop, test split\n",
    "\n",
    "![Train develop test](./img/train-develop-test.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755daf99-38d2-41dd-8ab1-cedce49ac7b0",
   "metadata": {},
   "source": [
    "- Decrease bias by making the model more complex, e.g. bigger network\n",
    "- Decrease variance with more data, regularisation, dropout, new architecture\n",
    "- The most common form of regularisation is L2 regularisation, so the cost function is\n",
    "$$\n",
    "\\begin{align}\n",
    "J &= -\\frac{1}{m} \\sum_{i=1}^my \\log(\\hat{y}^{i}) + (1-y^{i})\\log(1-\\hat{y}^{i}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L}\\|W^l\\|^2 \\\\\n",
    "J &= -\\frac{1}{m} \\sum_{i=1}^my \\log(\\hat{y}^{i}) + (1-y^{i})\\log(1-\\hat{y}^{i}) + \\frac{\\lambda}{2m} \n",
    " \\sum_{l=1}^{L}\\sum_{i=1}^{n_{l}}\\sum_{j=1}^{n_{l-1}}(W_{ij}^{[l]})^2 \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T) + \\frac{\\lambda}{2m}\\text{trace}(W^TW)\n",
    "\\end{align}\n",
    "$$\n",
    "- In dropout each node is dropped out based on parameter `keep_prob`. When implementing we introduce $D$ a matrix of 1s and 0s then drop out nodes by multiplying\n",
    "$$A^{[l]} = D^{[l]} * A^{[l]} / \\text{Keep Probabiliy }$$\n",
    "- Normalise inputs using strandard scaling to increase the speed of training a nueral network\n",
    "- Vanishing and exploding gradients happend due to many multiplications of terms in back prop. To avoid this use good initialisation with $$W^{[l]} = np.random.rand(shape) * \\sqrt(\\frac{2}{n^{[l-1]}})$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff965a0c-88e4-4c45-805f-c17e1e9bd9c5",
   "metadata": {},
   "source": [
    "### L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2aaf1d4-e0c3-4526-ad32-f070b67d8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def leaky_relu(x, leaky_constant:float = 0.01):\n",
    "    return np.where(x > 0.0, x,  x * leaky_constantÃ)\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "\n",
    "def log_loss(A, Y, Ws, regularisation_lambda:float = 0):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    cost = - (1/m) * ((Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T))\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(np.square(W).sum() for W in Ws.values())\n",
    "\n",
    "def square_loss(A, Y, Ws, regularisation_lambda:float = 0):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    cost = - (1/m) * ((A - Y) @ (A - Y).T)\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(np.square(W).sum() for W in Ws.values())\n",
    "\n",
    "ACTIVATION_FUNCTIONS : dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"log_loss\": log_loss,\n",
    "    \"square_loss\": square_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909d42a1-15f9-4154-a98e-18e96173600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As = {}, {0:X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "        if cache:\n",
    "            self.Zs, self.As = Zs, As\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1.0 / m) * dZs[l] @ self.As[l-1].T\n",
    "            if self.regularisation_lambda:\n",
    "                dWs[l] += ((self.regularisation_lambda / m) * self.Ws[l])\n",
    "            dbs[l] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False, fig=None):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name=f'{self.model_id}')\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "            cost = self.cost(A, Y)\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                costs.append(cost.item()) # extract from (1, 1) array\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs\n",
    "                        fig.data[-1].y = costs\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eccb5ee1-2dc9-496e-88ed-53950c4be854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f643453a0b814c4cacaaad3e736606f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train with no regularisation: 0.8328651685393258\n",
      "Accuracy on test with no regularisation: 0.8044692737430168\n",
      "Accuracy on train with regularisation: 0.827247191011236\n",
      "Accuracy on test with regularisation: 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def regularisation_example():\n",
    "\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    # Define a simple neural network architecture\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    # Fit with lambda = 0 (no regularization)\n",
    "    nn_no_reg = NeuralNetwork(layers, regularisation_lambda=0, model_id=\"No regularisation\")\n",
    "    nn_no_reg.initialise_weights()\n",
    "    nn_no_reg.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_no_reg = nn_no_reg.predict(X_train)\n",
    "    test_Y_pred_no_reg = nn_no_reg.predict(X_test)\n",
    "\n",
    "    # Fit with lambda = 0.2 (with regularization)\n",
    "    nn_reg = NeuralNetwork(layers, regularisation_lambda=1, model_id=\"With Regularisation\")\n",
    "    nn_reg.initialise_weights()\n",
    "    nn_reg.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_reg = nn_reg.predict(X_train)\n",
    "    test_Y_pred_reg = nn_reg.predict(X_test)\n",
    "\n",
    "    train_accuracy_no_reg = (train_Y_pred_no_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_no_reg = (test_Y_pred_no_reg == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_reg = (train_Y_pred_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_reg = (test_Y_pred_reg == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train with no regularisation: {train_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on test with no regularisation: {test_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on train with regularisation: {train_accuracy_reg}\")\n",
    "    print(f\"Accuracy on test with regularisation: {test_accuracy_reg}\")\n",
    "\n",
    "\n",
    "regularisation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc7504-902b-4ae7-8f42-8a25dd11c6eb",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "930dc3fd-f069-4f6c-be65-2a21dd0eb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0:X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(int) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                # dj/dz = dj/da * da/dz\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * self.Ds[l] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1.0 / m) * dZs[l] @ self.As[l-1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                dWs[l] += ((self.regularisation_lambda / m) * self.Ws[l])\n",
    "            dbs[l] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False, fig=None):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name=self.model_id)\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "            cost = self.cost(A, Y).item()\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                if np.isnan(cost):\n",
    "                    cost = 10\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs\n",
    "                        fig.data[-1].y = costs\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dd565f1-7892-493c-aa40-8047a3d10b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e533c21b1f6a4d18987f44cddf7affbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train with no regularisation: 0.8384831460674157\n",
      "Accuracy on test with no regularisation: 0.8044692737430168\n",
      "Accuracy on train with regularisation: 0.824438202247191\n",
      "Accuracy on test with regularisation: 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "def dropout_example():\n",
    "\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    # Define a simple neural network architecture\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    # Fit with lambda = 0 (no regularization)\n",
    "    nn_no_reg = NeuralNetwork(layers, keep_prob=1, model_id=\"No dropout\")\n",
    "    nn_no_reg.initialise_weights()\n",
    "    nn_no_reg.train(X_train, y_train, n_epochs=500, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_no_reg = nn_no_reg.predict(X_train)\n",
    "    test_Y_pred_no_reg = nn_no_reg.predict(X_test)\n",
    "\n",
    "    # Fit with lambda = 0.2 (with regularization)\n",
    "    nn_reg = NeuralNetwork(layers, keep_prob=0.8, model_id=\"20% drop out\")\n",
    "    nn_reg.initialise_weights()\n",
    "    nn_reg.train(X_train, y_train, n_epochs=500, log_every=50, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_reg = nn_reg.predict(X_train)\n",
    "    test_Y_pred_reg = nn_reg.predict(X_test)\n",
    "\n",
    "    train_accuracy_no_reg = (train_Y_pred_no_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_no_reg = (test_Y_pred_no_reg == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_reg = (train_Y_pred_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_reg = (test_Y_pred_reg == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train with no regularisation: {train_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on test with no regularisation: {test_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on train with regularisation: {train_accuracy_reg}\")\n",
    "    print(f\"Accuracy on test with regularisation: {test_accuracy_reg}\")\n",
    "\n",
    "\n",
    "dropout_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055148f-4770-4074-8549-680b3a1efba0",
   "metadata": {},
   "source": [
    "### Standard scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d4423e2-0ade-4c00-9a53-bda96bbde2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e2d59591aa4a3491fbda49b91e5ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.8328651685393258\n",
      "Accuracy on test: 0.8156424581005587\n",
      "Accuracy on train with scaled data: 0.8455056179775281\n",
      "Accuracy on test with scaled data: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.means: Optional[NDArray] = None\n",
    "        self.stds: Optional[NDArray] = None\n",
    "        self.is_fitted: bool = False\n",
    "\n",
    "    def fit_transform(self, X) -> NDArray:\n",
    "        self.means = X.mean(axis=1, keepdims=True)\n",
    "        self.stds = X.std(axis=1, keepdims=True)\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "    def transform(self, X) -> NDArray:\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "    def fit(self, X: NDArray) -> None:\n",
    "        self.means = X.mean(axis=1, keepdims=True)\n",
    "        self.stds = X.std(axis=1, keepdims=True)\n",
    "        self.is_fitted = True\n",
    "\n",
    "def standard_scaling_example():\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    nn = NeuralNetwork(layers, model_id=\"unscaled data\")\n",
    "    nn_scaled = NeuralNetwork(layers, model_id=\"scaled data\")\n",
    "    nn.initialise_weights()\n",
    "    nn_scaled.initialise_weights()\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "    nn.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    nn_scaled.train(X_train_scaled, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "\n",
    "    train_Y_pred = nn.predict(X_train)\n",
    "    test_Y_pred = nn.predict(X_test)\n",
    "    train_Y_pred_scaled = nn_scaled.predict(X_train_scaled)\n",
    "    test_Y_pred_scaled = nn_scaled.predict(X_test_scaled)\n",
    "\n",
    "    train_accuracy = (train_Y_pred == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy = (test_Y_pred == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_scaled = (train_Y_pred_scaled == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_scaled = (test_Y_pred_scaled == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train: {train_accuracy}\")\n",
    "    print(f\"Accuracy on test: {test_accuracy}\")\n",
    "    print(f\"Accuracy on train with scaled data: {train_accuracy_scaled}\")\n",
    "    print(f\"Accuracy on test with scaled data: {test_accuracy_scaled}\")\n",
    "\n",
    "standard_scaling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ef4da-66b2-4b89-b0f7-719e2ede8fe8",
   "metadata": {},
   "source": [
    "### Mini-batch training, Momentum, RMSprop and Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e7d7d",
   "metadata": {},
   "source": [
    " - In mini-batch gradient descent, we use a subset of the training data to update the weights.\n",
    " - In batch gradient descent, we use all the training data to update the weights.\n",
    " - In stochastic gradient descent, we use a single training example to update the weights.\n",
    " - The momentum optimisation algorithm uses a moving average of the gradients to update the weights. Momentum helps by enabling optimizers to overcome local minima and saddle points, reduce oscillations in narrow valleys for a smoother path, and speed up learning by maintaining a consistent direction through flat or consistent gradient regions.\n",
    " $$\n",
    " \\begin{align*}\n",
    " v_t &= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla J(w) \\\\\n",
    " w &= w - \\alpha v_t\n",
    " \\end{align*}\n",
    " $$ where $v_t$ is the moving average of the gradients, $\\beta_1$ is the momentum coefficient, $\\nabla J(w)$ is the gradient of the cost function $J(w)$ with respect to the weights $w$, and $\\alpha$ is the learning rate.\n",
    " - The RMSprop optimisation algorithm maintains a moving average of the squared gradients, it then updates the weights using the gradients divided by the square root of the moving average. RMSprop adaptively adjusts learning rates: it increases them for parameters with small gradients (in flat areas) and decreases them for those with large gradients (in steep or oscillating areas), balancing updates. This method also normalizes gradient magnitudes, preventing them from becoming too small (vanishing) or too large (exploding), which leads to more stable and efficient training.\n",
    " $$\n",
    " \\begin{align*}\n",
    " s_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(w))^2 \\\\\n",
    " w &= w - \\alpha \\frac{\\nabla J(w)}{\\sqrt{s_t + \\epsilon}}\n",
    " \\end{align*}\n",
    " $$ where $s_t$ is the moving average of the squared gradients, $\\beta_2$ is the RMSprop coefficient, $\\nabla J(w)$ is the gradient of the cost function $J(w)$ with respect to the weights $w$, and $\\alpha$ is the learning rate.\n",
    " - The adam algorihtm combines the momentum and RMSprop optimisation algorithms. It uses a moving average of the gradients scaled by a moving average of the squared gradients. Adam also uses a bias correction term to adjust the moving averages.\n",
    " $$\n",
    " \\begin{align*}\n",
    " v_t &= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla J(w) \\\\\n",
    " s_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(w))^2 \\\\\n",
    " \\hat{v_t} &= \\frac{v_t}{1 - \\beta_1^t} \\\\\n",
    " \\hat{s_t} &= \\frac{s_t}{1 - \\beta_2^t} \\\\\n",
    " w &= w - \\alpha \\frac{\\hat{v_t}}{\\sqrt{\\hat{s_t} + \\epsilon}}\n",
    " \\end{align*}\n",
    " $$ where $v_t$ is the moving average of the gradients, $s_t$ is the moving average of the squared gradients, $\\beta_1$ and $\\beta_2$ are the momentum and RMSprop coefficients, $\\nabla J(w)$ is the gradient of the cost function $J(w)$ with respect to the weights $w$, and $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b79d59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bff363dc45b48c8abb3d5483528a7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: Gradient Descent\n",
      "Gradient Descent training time: 3.1561 seconds\n",
      "Accuracy on train: 0.8356741573033708\n",
      "Accuracy on test: 0.8044692737430168\n",
      "\n",
      " Optimiser: Mini-batch\n",
      "Mini-batch training time: 3.6310 seconds\n",
      "Accuracy on train: 0.8356741573033708\n",
      "Accuracy on test: 0.8100558659217877\n",
      "\n",
      " Optimiser: Momentum\n",
      "Momentum training time: 4.1299 seconds\n",
      "Accuracy on train: 0.8356741573033708\n",
      "Accuracy on test: 0.8044692737430168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/7sr7n0x97jqbhl4pqwv79vdw0000gn/T/ipykernel_10477/328717897.py:28: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "/var/folders/0f/7sr7n0x97jqbhl4pqwv79vdw0000gn/T/ipykernel_10477/328717897.py:28: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in matmul\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: RMSProp\n",
      "RMSProp training time: 4.7266 seconds\n",
      "Accuracy on train: 0.9185393258426966\n",
      "Accuracy on test: 0.7932960893854749\n",
      "\n",
      " Optimiser: ADAM\n",
      "ADAM training time: 5.1561 seconds\n",
      "Accuracy on train: 0.9410112359550562\n",
      "Accuracy on test: 0.8212290502793296\n",
      "\n",
      " Optimiser: ADAM with learning rate decay\n",
      "ADAM with learning rate decay training time: 5.5480 seconds\n",
      "Accuracy on train: 0.8469101123595506\n",
      "Accuracy on test: 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l: \"sigmoid\" for l in range(1, self.L)} | {\n",
    "                self.L: \"sigmoid\"\n",
    "            }\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.params = {}\n",
    "        for l, (n_l, n_l_minus_1) in enumerate(\n",
    "            zip(self.layer_sizes[1:], self.layer_sizes), start=1\n",
    "        ):\n",
    "            self.params[f\"W{l}\"] = np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(\n",
    "                2 / n_l_minus_1\n",
    "            )\n",
    "            self.params[f\"b{l}\"] = np.zeros((n_l, 1))\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.params=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0: X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            W = self.params[f\"W{l}\"]\n",
    "            b = self.params[f\"b{l}\"]\n",
    "            Zs[l] = W @ As[l - 1] + b\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out but not on the output layer\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(\n",
    "                    int\n",
    "                ) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y):\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        grads = {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                W_l_plus_1 = self.params[f\"W{l+1}\"]\n",
    "                dZs[l] = (\n",
    "                    W_l_plus_1.T\n",
    "                    @ dZs[l + 1]\n",
    "                    * self.Ds[l]\n",
    "                    * ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](\n",
    "                        self.Zs[l]\n",
    "                    )\n",
    "                )\n",
    "            grads[f\"dW{l}\"] = (1.0 / m) * dZs[l] @ self.As[l - 1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                grads[f\"dW{l}\"] += (self.regularisation_lambda / m) * self.params[\n",
    "                    f\"W{l}\"\n",
    "                ]\n",
    "            grads[f\"db{l}\"] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def cost(self, A, Y) -> float:\n",
    "        # For cost, we need to pass the weights. We'll extract them from self.params.\n",
    "        Ws = {l: self.params[f\"W{l}\"] for l in range(1, self.L + 1)}\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Incorrect value for self.cost_function:= {self.cost_function}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"This implements gradient descent. With optional mini-batching.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.1, batch_size: int | None = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        \"\"\"Update model parameters using grads returned from backward.\"\"\"\n",
    "        for param_key in model.params:\n",
    "            model.params[param_key] -= self.learning_rate * grads[f\"d{param_key}\"]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        n_epochs=1000,\n",
    "        log_every: int | None = None,\n",
    "        plot_cost=False,\n",
    "        fig=None,\n",
    "        plot_every=10,\n",
    "    ):\n",
    "        costs, epochs = [], []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode=\"lines+markers\", name=model.model_id)\n",
    "            fig.update_layout(\n",
    "                title=\"Training Cost over Epochs\",\n",
    "                xaxis_title=\"Epoch\",\n",
    "                yaxis_title=\"Cost\",\n",
    "            )\n",
    "\n",
    "        m = X.shape[1]\n",
    "        batch_size = self.batch_size if self.batch_size is not None else m\n",
    "\n",
    "        training_iteration = 1\n",
    "        for epoch in range(n_epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X[:, i : i + batch_size]\n",
    "                Y_batch = Y[:, i : i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                A = model.forward(X_batch, cache=True)\n",
    "                grads = model.backward(Y_batch)\n",
    "                self.update_model_params(model, grads, training_iteration)\n",
    "                training_iteration += 1\n",
    "\n",
    "            # Compute cost on the whole dataset after epoch\n",
    "            A_full = model.forward(X, cache=False)\n",
    "            cost = model.cost(A_full, Y)\n",
    "            if log_every and epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost and fig is not None:\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch + 1)\n",
    "                if epoch % plot_every == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs  # type: ignore\n",
    "                        fig.data[-1].y = costs  # type: ignore\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    \"\"\"Implements momentum optimizer.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta * v_{t-1} + grad_t\n",
    "    param_t = param_{t-1} - learning_rate * v_t\n",
    "\n",
    "    Where:\n",
    "    - S_t is the second moment of the gradient\n",
    "    - param_t is the parameter\n",
    "    - grad_t is the gradient of the cost function with respect to the parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.cache:\n",
    "                self.cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.cache[param_key] = (\n",
    "                self.beta * self.cache[param_key]\n",
    "                + (1 - self.beta) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            model.params[param_key] -= self.learning_rate * self.cache[param_key]\n",
    "\n",
    "\n",
    "class RMSPropOptimizer(Optimizer):\n",
    "    \"\"\"Implements RMSProp optimizer.\n",
    "\n",
    "    Here we track the exponentially weighted average of the squared gradients (second moment).\n",
    "\n",
    "    The update rule is:\n",
    "    S_t = beta * S_{t-1} + (1 - beta) * (grad_t)^2\n",
    "    param_t = param_{t-1} - learning_rate * grad_t / (sqrt(S_t) + epsilon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        # Standard RMSProp does NOT use bias correction (unlike Adam)\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.s_cache:\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.s_cache[param_key] = self.beta * self.s_cache[param_key] + (\n",
    "                1 - self.beta\n",
    "            ) * (grads[f\"d{param_key}\"] ** 2)\n",
    "            model.params[param_key] -= (\n",
    "                self.learning_rate\n",
    "                * grads[f\"d{param_key}\"]\n",
    "                / (np.sqrt(self.s_cache[param_key]) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "class ADAMOptimizer(Optimizer):\n",
    "    \"\"\"Implements ADAM optimizer with learning rate decay.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta_1 * v_{t-1} + (1 - beta_1) * grad\n",
    "    s_t = beta_2 * s_{t-1} + (1 - beta_2) * (grad_t)^2\n",
    "    v_t_corrected = v_t / (1 - beta_1^t)\n",
    "    s_t_corrected = s_t / (1 - beta_2^t)\n",
    "    param_t = param_{t-1} - learning_rate * v_t_corrected / (sqrt(s_t_corrected) + epsilon)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.999,\n",
    "        epsilon: float = 1e-8,\n",
    "        learning_rate_decay: float = 0,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta_1 = beta_1  # for the momentum\n",
    "        self.beta_2 = beta_2  # for the second moment (RMSProp)\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.v_cache = {}\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        learning_rate = self.learning_rate * (\n",
    "            1 / (1 + self.learning_rate_decay * training_iteration)\n",
    "        )\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.v_cache:\n",
    "                self.v_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.v_cache[param_key] = (\n",
    "                self.beta_1 * self.v_cache[param_key]\n",
    "                + (1 - self.beta_1) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            self.s_cache[param_key] = (\n",
    "                self.beta_2 * self.s_cache[param_key]\n",
    "                + (1 - self.beta_2) * grads[f\"d{param_key}\"] ** 2\n",
    "            )\n",
    "            v_t_corrected = self.v_cache[param_key] / (\n",
    "                1 - self.beta_1**training_iteration\n",
    "            )\n",
    "            s_t_corrected = self.s_cache[param_key] / (\n",
    "                1 - self.beta_2**training_iteration\n",
    "            )\n",
    "\n",
    "            model.params[param_key] -= (\n",
    "                learning_rate * v_t_corrected / (np.sqrt(s_t_corrected) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "def optimizer_example():\n",
    "    X_train = pd.read_feather(\"../titanic/processed/X_train.feather\").to_numpy().T\n",
    "    y_train = pd.read_feather(\"../titanic/processed/y_train.feather\").to_numpy().T\n",
    "    X_test = pd.read_feather(\"../titanic/processed/X_test.feather\").to_numpy().T\n",
    "    y_test = pd.read_feather(\"../titanic/processed/y_test.feather\").to_numpy().T\n",
    "\n",
    "    layers = [30, 50, 20, 1]\n",
    "    nn = NeuralNetwork(layers, model_id=\"Using an optimizer\")\n",
    "    nn_mini_batch = NeuralNetwork(layers, model_id=\"Using a mini-batch optimizer\")\n",
    "    nn_momentum = NeuralNetwork(layers, model_id=\"Using a momentum optimizer\")\n",
    "    nn_rmsprop = NeuralNetwork(layers, model_id=\"Using a RMSProp optimizer\")\n",
    "    nn_adam = NeuralNetwork(layers, model_id=\"Using a ADAM optimizer\")\n",
    "    nn_adam_decay = NeuralNetwork(\n",
    "        layers, model_id=\"Using a ADAM optimizer with learning rate decay\"\n",
    "    )\n",
    "\n",
    "    nn.initialise_weights()\n",
    "    nn_mini_batch.initialise_weights()\n",
    "    nn_momentum.initialise_weights()\n",
    "    nn_rmsprop.initialise_weights()\n",
    "    nn_adam.initialise_weights()\n",
    "    nn_adam_decay.initialise_weights()\n",
    "\n",
    "    optimizer = Optimizer(learning_rate=0.5)\n",
    "    mini_batch_optimizer = Optimizer(batch_size=128)\n",
    "    momentum_optimizer = MomentumOptimizer(batch_size=128)\n",
    "    rmsprop_optimizer = RMSPropOptimizer(batch_size=128)\n",
    "    adam_optimizer = ADAMOptimizer(batch_size=128)\n",
    "    adam_decay_optimizer = ADAMOptimizer(batch_size=128, learning_rate_decay=0.5)\n",
    "\n",
    "    models = [nn, nn_mini_batch, nn_momentum, nn_rmsprop, nn_adam, nn_adam_decay]\n",
    "    optimizers = [\n",
    "        optimizer,\n",
    "        mini_batch_optimizer,\n",
    "        momentum_optimizer,\n",
    "        rmsprop_optimizer,\n",
    "        adam_optimizer,\n",
    "        adam_decay_optimizer,\n",
    "    ]\n",
    "    names = [\n",
    "        \"Gradient Descent\",\n",
    "        \"Mini-batch\",\n",
    "        \"Momentum\",\n",
    "        \"RMSProp\",\n",
    "        \"ADAM\",\n",
    "        \"ADAM with learning rate decay\",\n",
    "    ]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    from time import perf_counter\n",
    "\n",
    "    for model, optimizer, name in zip(models, optimizers, names):\n",
    "        start_time = perf_counter()\n",
    "        optimizer.train(model, X_train, y_train, n_epochs=500, plot_cost=True, fig=fig)\n",
    "        end_time = perf_counter()\n",
    "        time = end_time - start_time\n",
    "        print(f\"\\n Optimiser: {name}\")\n",
    "        print(f\"{name} training time: {time:.4f} seconds\")\n",
    "        train_Y_pred = model.predict(X_train)\n",
    "        test_Y_pred = model.predict(X_test)\n",
    "        train_accuracy = (train_Y_pred == y_train).sum() / y_train.shape[1]\n",
    "        test_accuracy = (test_Y_pred == y_test).sum() / y_test.shape[1]\n",
    "        print(f\"Accuracy on train: {train_accuracy}\")\n",
    "        print(f\"Accuracy on test: {test_accuracy}\")\n",
    "\n",
    "\n",
    "optimizer_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b30592",
   "metadata": {},
   "source": [
    "### Softmax regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79934143",
   "metadata": {},
   "source": [
    "\n",
    "- In the case when there are C classes we want the model to output a probability distribution over the C classes. To acheive this we use C units in the output layer and apply the softmax function to the output of the last layer.\n",
    "$$\n",
    " \\begin{align*}\n",
    " \\hat{y} &= \\text{softmax}(z) \\\\\n",
    " \\hat{y}_j &= \\frac{e^{z_j}}{\\sum_{c=1}^{C} e^{z_c}}\n",
    " \\end{align*}\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e48fea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e5e36201cd449fb3bab1397a59b9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: <__main__.ADAMOptimizer object at 0x1394624d0>\n",
      "Training time: 0.1505 seconds\n",
      "train_Y_pred_classes: [0 0 2 0 0 0 0 2 2 2 0 2 1 2 1 0 2 1 0 2 1 1 2 2 0 2 1 1 0 1 2 1 1 1 0 2 0\n",
      " 0 2 0 0 1 0 2 1 0 2 1 2 0 0 1 2 0 2 0 0 1 0 1 2 1 1 2 0 0 0 0 1 0 2 2 1 1\n",
      " 1 0 0 1 0 1 2 2 1 1 1 2 1 0 1 1 1 2 1 1 2 1 2 1 1 2 0 0 1 2 2 1 0 2 2 1 2\n",
      " 2 0 2 2 0 0 2 0 2]\n",
      "Accuracy on train: 0.825\n",
      "Accuracy on test: 0.8\n",
      "Accuracy on train: 0.825\n",
      "Accuracy on test: 0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "len": 1,
          "tickmode": "array",
          "ticktext": [
           "setosa",
           "versicolor",
           "virginica"
          ],
          "tickvals": [
           0,
           1,
           2
          ],
          "title": {
           "text": "Predicted Class"
          },
          "y": 1,
          "yanchor": "top"
         },
         "colorscale": [
          [
           0,
           "rgb(255,0,0)"
          ],
          [
           0.5,
           "rgb(0,0,255)"
          ],
          [
           1,
           "rgb(0,255,0)"
          ]
         ],
         "contours": {
          "showlabels": true
         },
         "opacity": 0.4,
         "type": "contour",
         "x": [
          3.8,
          3.844444444444444,
          3.888888888888889,
          3.933333333333333,
          3.9777777777777774,
          4.022222222222222,
          4.066666666666666,
          4.111111111111111,
          4.155555555555555,
          4.2,
          4.2444444444444445,
          4.288888888888889,
          4.333333333333333,
          4.377777777777777,
          4.422222222222222,
          4.466666666666667,
          4.511111111111111,
          4.555555555555555,
          4.6,
          4.644444444444444,
          4.688888888888888,
          4.733333333333333,
          4.777777777777778,
          4.822222222222222,
          4.866666666666666,
          4.9111111111111105,
          4.955555555555556,
          5,
          5.044444444444444,
          5.088888888888889,
          5.133333333333333,
          5.177777777777777,
          5.222222222222221,
          5.266666666666667,
          5.311111111111111,
          5.355555555555555,
          5.3999999999999995,
          5.444444444444444,
          5.488888888888889,
          5.533333333333333,
          5.5777777777777775,
          5.622222222222222,
          5.666666666666666,
          5.71111111111111,
          5.755555555555555,
          5.8,
          5.844444444444444,
          5.888888888888888,
          5.933333333333333,
          5.977777777777778,
          6.022222222222222,
          6.066666666666666,
          6.111111111111111,
          6.155555555555555,
          6.199999999999999,
          6.244444444444444,
          6.288888888888888,
          6.333333333333333,
          6.377777777777777,
          6.422222222222222,
          6.466666666666667,
          6.511111111111111,
          6.555555555555555,
          6.6,
          6.644444444444444,
          6.688888888888888,
          6.7333333333333325,
          6.777777777777777,
          6.822222222222221,
          6.866666666666666,
          6.9111111111111105,
          6.955555555555555,
          7,
          7.044444444444444,
          7.088888888888889,
          7.133333333333333,
          7.177777777777777,
          7.222222222222221,
          7.266666666666666,
          7.31111111111111,
          7.355555555555555,
          7.3999999999999995,
          7.444444444444444,
          7.488888888888889,
          7.533333333333333,
          7.5777777777777775,
          7.622222222222222,
          7.666666666666666,
          7.71111111111111,
          7.755555555555555,
          7.799999999999999,
          7.844444444444444,
          7.888888888888888,
          7.933333333333333,
          7.977777777777777,
          8.022222222222222,
          8.066666666666666,
          8.11111111111111,
          8.155555555555555,
          8.2
         ],
         "xaxis": "x",
         "y": [
          1.5,
          1.5323232323232323,
          1.5646464646464646,
          1.596969696969697,
          1.6292929292929292,
          1.6616161616161615,
          1.6939393939393939,
          1.7262626262626264,
          1.7585858585858585,
          1.790909090909091,
          1.823232323232323,
          1.8555555555555556,
          1.887878787878788,
          1.9202020202020202,
          1.9525252525252526,
          1.9848484848484849,
          2.017171717171717,
          2.0494949494949495,
          2.081818181818182,
          2.114141414141414,
          2.146464646464646,
          2.1787878787878787,
          2.2111111111111112,
          2.2434343434343433,
          2.275757575757576,
          2.308080808080808,
          2.3404040404040405,
          2.3727272727272726,
          2.405050505050505,
          2.437373737373737,
          2.4696969696969697,
          2.5020202020202023,
          2.5343434343434343,
          2.5666666666666664,
          2.598989898989899,
          2.6313131313131315,
          2.6636363636363636,
          2.6959595959595957,
          2.728282828282828,
          2.7606060606060607,
          2.792929292929293,
          2.8252525252525253,
          2.8575757575757574,
          2.88989898989899,
          2.9222222222222225,
          2.9545454545454546,
          2.9868686868686867,
          3.019191919191919,
          3.0515151515151517,
          3.083838383838384,
          3.116161616161616,
          3.1484848484848484,
          3.180808080808081,
          3.213131313131313,
          3.245454545454545,
          3.2777777777777777,
          3.31010101010101,
          3.3424242424242427,
          3.374747474747475,
          3.407070707070707,
          3.4393939393939394,
          3.471717171717172,
          3.504040404040404,
          3.536363636363636,
          3.5686868686868687,
          3.601010101010101,
          3.6333333333333333,
          3.665656565656566,
          3.697979797979798,
          3.7303030303030305,
          3.7626262626262625,
          3.794949494949495,
          3.827272727272727,
          3.8595959595959597,
          3.891919191919192,
          3.9242424242424243,
          3.9565656565656564,
          3.988888888888889,
          4.0212121212121215,
          4.053535353535354,
          4.085858585858586,
          4.118181818181818,
          4.150505050505051,
          4.182828282828282,
          4.215151515151515,
          4.247474747474747,
          4.27979797979798,
          4.3121212121212125,
          4.344444444444445,
          4.376767676767677,
          4.409090909090909,
          4.441414141414141,
          4.473737373737373,
          4.506060606060606,
          4.538383838383838,
          4.570707070707071,
          4.6030303030303035,
          4.635353535353536,
          4.667676767676768,
          4.7
         ],
         "yaxis": "y",
         "z": [
          [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           1,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ],
          [
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2,
           2
          ]
         ]
        },
        {
         "marker": {
          "color": "red",
          "size": 8
         },
         "mode": "markers",
         "name": "setosa",
         "showlegend": true,
         "type": "scatter",
         "x": [
          5.1,
          5,
          4.6,
          4.9,
          5.1,
          4.4,
          5.2,
          5,
          5,
          5,
          5.4,
          5,
          4.3,
          5,
          4.8,
          5.5,
          4.9,
          5.1,
          4.9,
          4.8,
          5.3,
          5.1,
          4.7,
          4.6,
          4.7,
          5.1,
          4.5,
          5.1,
          5,
          4.9,
          5.4,
          5.7,
          5.4,
          5.2,
          4.6,
          5.4,
          5,
          4.8,
          4.4,
          4.6
         ],
         "xaxis": "x",
         "y": [
          3.5,
          3.5,
          3.1,
          3.1,
          3.8,
          3,
          3.4,
          3.2,
          3.5,
          3.6,
          3.7,
          3,
          3,
          3.4,
          3.1,
          4.2,
          3,
          3.7,
          3.6,
          3,
          3.7,
          3.8,
          3.2,
          3.2,
          3.2,
          3.4,
          2.3,
          3.3,
          3.4,
          3.1,
          3.4,
          3.8,
          3.9,
          3.5,
          3.6,
          3.9,
          3.3,
          3.4,
          3.2,
          3.4
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "blue",
          "size": 8
         },
         "mode": "markers",
         "name": "versicolor",
         "showlegend": true,
         "type": "scatter",
         "x": [
          6.3,
          5.4,
          6,
          6.4,
          5.8,
          5.6,
          6.1,
          6.8,
          5.7,
          6.4,
          5.8,
          6.9,
          5,
          6,
          5.6,
          5.6,
          5.5,
          6.3,
          5.6,
          6.1,
          7,
          5.5,
          6.1,
          6.2,
          6,
          6,
          5.5,
          5.8,
          5.5,
          5.6,
          5.7,
          5.2,
          5.7,
          5.9,
          5,
          6.6,
          5.1,
          6.5,
          6.1,
          6.6,
          6.7
         ],
         "xaxis": "x",
         "y": [
          2.3,
          3,
          3.4,
          3.2,
          2.6,
          2.9,
          3,
          2.8,
          2.9,
          2.9,
          2.7,
          3.1,
          2.3,
          2.2,
          3,
          2.7,
          2.6,
          2.5,
          2.5,
          2.8,
          3.2,
          2.4,
          2.8,
          2.9,
          2.7,
          2.9,
          2.5,
          2.7,
          2.4,
          3,
          2.8,
          2.7,
          2.8,
          3.2,
          2,
          2.9,
          2.5,
          2.8,
          2.9,
          3,
          3
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "green",
          "size": 8
         },
         "mode": "markers",
         "name": "virginica",
         "showlegend": true,
         "type": "scatter",
         "x": [
          7.2,
          7.6,
          6.3,
          6.7,
          7.2,
          5.7,
          6.3,
          6.3,
          6.7,
          5.8,
          6.4,
          6.8,
          6,
          5.8,
          6.4,
          6.4,
          7.7,
          6.4,
          6.7,
          6,
          6.9,
          7.7,
          6.3,
          7.7,
          5.8,
          6.8,
          7.1,
          7.3,
          5.9,
          6.1,
          6.5,
          7.2,
          6.3,
          6.1,
          6.5,
          6.3,
          6.5,
          6.7,
          6.4
         ],
         "xaxis": "x",
         "y": [
          3,
          3,
          2.7,
          3.1,
          3.2,
          2.5,
          2.9,
          2.8,
          3,
          2.7,
          3.2,
          3,
          2.2,
          2.7,
          3.1,
          2.7,
          3.8,
          2.8,
          3.3,
          3,
          3.1,
          3,
          3.3,
          2.8,
          2.8,
          3.2,
          3,
          2.9,
          3,
          3,
          3,
          3.6,
          2.5,
          2.6,
          3,
          3.4,
          3,
          2.5,
          2.8
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "sepal length vs sepal width",
          "x": 0.225,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "legend": {
         "title": {
          "text": "Actual Classes"
         },
         "x": 1.05,
         "xanchor": "left",
         "y": 0.99,
         "yanchor": "top"
        },
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Decision Boundaries for All Feature Pairs"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.45
         ],
         "title": {
          "text": "sepal length"
         }
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.55,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "sepal width"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Callable\n",
    "import logging\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1.0 * (x > 0)\n",
    "\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "\n",
    "def leaky_relu(x, leaky_constant: float = 0.01):\n",
    "    return np.where(x > 0.0, x, x * leaky_constant)\n",
    "\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "\n",
    "def log_loss(A, Y, Ws, regularisation_lambda: float = 0):\n",
    "    m = A.shape[1]  # m is number samples\n",
    "    cost = -(1 / m) * (Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T)\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(\n",
    "        np.square(W).sum() for W in Ws.values()\n",
    "    )\n",
    "\n",
    "\n",
    "def log_loss(A, Y, Ws, regularisation_lambda: float = 0):\n",
    "    m = A.shape[1]  # m is number of samples\n",
    "    # Clip values to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    A = np.clip(A, epsilon, 1 - epsilon)\n",
    "\n",
    "    # For multi-class (softmax), we use categorical cross-entropy\n",
    "    if A.shape[0] > 1:  # If output dimension > 1, assume multi-class\n",
    "        # Categorical cross-entropy: -1/m * sum(y_i * log(a_i))\n",
    "        cost = -np.sum(Y * np.log(A)) / m\n",
    "    else:  # Binary classification case\n",
    "        cost = -(1 / m) * (Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T)\n",
    "\n",
    "    # Add L2 regularization if specified\n",
    "    if regularisation_lambda:\n",
    "        l2_cost = (regularisation_lambda / (2 * m)) * sum(\n",
    "            np.square(W).sum() for W in Ws.values()\n",
    "        )\n",
    "        cost += l2_cost\n",
    "\n",
    "    return np.array([[cost]])\n",
    "\n",
    "\n",
    "def square_loss(A, Y, Ws, regularisation_lambda: float = 0):\n",
    "    m = A.shape[1]  # m is number samples\n",
    "    cost = -(1 / m) * ((A - Y) @ (A - Y).T)\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(\n",
    "        np.square(W).sum() for W in Ws.values()\n",
    "    )\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    # if Z has shape (C, m) then we want to return a matrix of shape (C, m)\n",
    "    # Note the softmax is applied for each column of Z\n",
    "    stable_z = z - np.max(z, axis=0, keepdims=True)\n",
    "    exponents = np.exp(stable_z)\n",
    "    probabilities = exponents / np.sum(exponents, axis=0, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "ACTIVATION_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "    \"softmax\": softmax,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\"log_loss\": log_loss, \"square_loss\": square_loss}\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"This implements a simple feedforward neural network.\n",
    "\n",
    "    This additionally allows for dropout and regularisation and\n",
    "    multi class output via the softmax function.\n",
    "    \"\"\"\n",
    "\n",
    "    layer_activations: dict[int, str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l: \"sigmoid\" for l in range(1, self.L)} | {\n",
    "                self.L: \"sigmoid\"\n",
    "            }\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.params = {}\n",
    "        for l, (n_l, n_l_minus_1) in enumerate(\n",
    "            zip(self.layer_sizes[1:], self.layer_sizes), start=1\n",
    "        ):\n",
    "            self.params[f\"W{l}\"] = np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(\n",
    "                2 / n_l_minus_1\n",
    "            )\n",
    "            self.params[f\"b{l}\"] = np.zeros((n_l, 1))\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.params=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0: X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            W = self.params[f\"W{l}\"]\n",
    "            b = self.params[f\"b{l}\"]\n",
    "            Zs[l] = W @ As[l - 1] + b\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out but not on the output layer\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(\n",
    "                    int\n",
    "                ) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y):\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        grads = {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                W_l_plus_1 = self.params[f\"W{l + 1}\"]\n",
    "                dZs[l] = (\n",
    "                    W_l_plus_1.T\n",
    "                    @ dZs[l + 1]\n",
    "                    * self.Ds[l]\n",
    "                    * ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](\n",
    "                        self.Zs[l]\n",
    "                    )\n",
    "                )\n",
    "            grads[f\"dW{l}\"] = (1.0 / m) * dZs[l] @ self.As[l - 1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                grads[f\"dW{l}\"] += (self.regularisation_lambda / m) * self.params[\n",
    "                    f\"W{l}\"\n",
    "                ]\n",
    "            grads[f\"db{l}\"] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def cost(self, A, Y) -> float:\n",
    "        # For cost, we need to pass the weights. We'll extract them from self.params.\n",
    "        Ws = {l: self.params[f\"W{l}\"] for l in range(1, self.L + 1)}\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Incorrect value for self.cost_function:= {self.cost_function}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"This implements gradient descent. With optional mini-batching.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.1, batch_size: int | None = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        \"\"\"Update model parameters using grads returned from backward.\"\"\"\n",
    "        for param_key in model.params:\n",
    "            model.params[param_key] -= self.learning_rate * grads[f\"d{param_key}\"]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        n_epochs=1000,\n",
    "        log_every: int | None = None,\n",
    "        plot_cost=False,\n",
    "        fig=None,\n",
    "        plot_every=10,\n",
    "    ):\n",
    "        costs, epochs = [], []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode=\"lines+markers\", name=model.model_id)\n",
    "            fig.update_layout(\n",
    "                title=\"Training Cost over Epochs\",\n",
    "                xaxis_title=\"Epoch\",\n",
    "                yaxis_title=\"Cost\",\n",
    "            )\n",
    "\n",
    "        m = X.shape[1]\n",
    "        batch_size = self.batch_size if self.batch_size is not None else m\n",
    "\n",
    "        training_iteration = 1\n",
    "        for epoch in range(n_epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X[:, i : i + batch_size]\n",
    "                Y_batch = Y[:, i : i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                A = model.forward(X_batch, cache=True)\n",
    "                grads = model.backward(Y_batch)\n",
    "                self.update_model_params(model, grads, training_iteration)\n",
    "                training_iteration += 1\n",
    "\n",
    "            # Compute cost on the whole dataset after epoch\n",
    "            A_full = model.forward(X, cache=False)\n",
    "            cost = model.cost(A_full, Y)\n",
    "            if log_every and epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost and fig is not None:\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch + 1)\n",
    "                if epoch % plot_every == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs  # type: ignore\n",
    "                        fig.data[-1].y = costs  # type: ignore\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    \"\"\"Implements momentum optimizer.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta * v_{t-1} + grad_t\n",
    "    param_t = param_{t-1} - learning_rate * v_t\n",
    "\n",
    "    Where:\n",
    "    - S_t is the second moment of the gradient\n",
    "    - param_t is the parameter\n",
    "    - grad_t is the gradient of the cost function with respect to the parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.cache:\n",
    "                self.cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.cache[param_key] = (\n",
    "                self.beta * self.cache[param_key]\n",
    "                + (1 - self.beta) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            model.params[param_key] -= self.learning_rate * self.cache[param_key]\n",
    "\n",
    "\n",
    "class RMSPropOptimizer(Optimizer):\n",
    "    \"\"\"Implements RMSProp optimizer.\n",
    "\n",
    "    Here we track the exponentially weighted average of the squared gradients (second moment).\n",
    "\n",
    "    The update rule is:\n",
    "    S_t = beta * S_{t-1} + (1 - beta) * (grad_t)^2\n",
    "    param_t = param_{t-1} - learning_rate * grad_t / (sqrt(S_t) + epsilon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        # Standard RMSProp does NOT use bias correction (unlike Adam)\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.s_cache:\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.s_cache[param_key] = self.beta * self.s_cache[param_key] + (\n",
    "                1 - self.beta\n",
    "            ) * (grads[f\"d{param_key}\"] ** 2)\n",
    "            model.params[param_key] -= (\n",
    "                self.learning_rate\n",
    "                * grads[f\"d{param_key}\"]\n",
    "                / (np.sqrt(self.s_cache[param_key]) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "class ADAMOptimizer(Optimizer):\n",
    "    \"\"\"Implements ADAM optimizer with learning rate decay.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta_1 * v_{t-1} + (1 - beta_1) * grad\n",
    "    s_t = beta_2 * s_{t-1} + (1 - beta_2) * (grad_t)^2\n",
    "    v_t_corrected = v_t / (1 - beta_1^t)\n",
    "    s_t_corrected = s_t / (1 - beta_2^t)\n",
    "    param_t = param_{t-1} - learning_rate * v_t_corrected / (sqrt(s_t_corrected) + epsilon)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.999,\n",
    "        epsilon: float = 1e-8,\n",
    "        learning_rate_decay: float = 0,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta_1 = beta_1  # for the momentum\n",
    "        self.beta_2 = beta_2  # for the second moment (RMSProp)\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.v_cache = {}\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        learning_rate = self.learning_rate * (\n",
    "            1 / (1 + self.learning_rate_decay * training_iteration)\n",
    "        )\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.v_cache:\n",
    "                self.v_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.v_cache[param_key] = (\n",
    "                self.beta_1 * self.v_cache[param_key]\n",
    "                + (1 - self.beta_1) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            self.s_cache[param_key] = (\n",
    "                self.beta_2 * self.s_cache[param_key]\n",
    "                + (1 - self.beta_2) * grads[f\"d{param_key}\"] ** 2\n",
    "            )\n",
    "            v_t_corrected = self.v_cache[param_key] / (\n",
    "                1 - self.beta_1**training_iteration\n",
    "            )\n",
    "            s_t_corrected = self.s_cache[param_key] / (\n",
    "                1 - self.beta_2**training_iteration\n",
    "            )\n",
    "\n",
    "            model.params[param_key] -= (\n",
    "                learning_rate * v_t_corrected / (np.sqrt(s_t_corrected) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model, X, y, resolution=0.02):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary for a model trained on all possible pairs of features.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import itertools\n",
    "\n",
    "    n_features = X.shape[0]\n",
    "    feature_names = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
    "    class_names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "    colors = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "    feature_pairs = list(itertools.combinations(range(n_features), 2))\n",
    "    n_pairs = len(feature_pairs)\n",
    "    n_rows = (n_pairs + 1) // 2\n",
    "    n_cols = 2\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows,\n",
    "        cols=n_cols,\n",
    "        subplot_titles=[\n",
    "            f\"{feature_names[i]} vs {feature_names[j]}\" for i, j in feature_pairs\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    y_classes = np.argmax(y, axis=0)\n",
    "    mean_features = X.mean(axis=1)\n",
    "\n",
    "    for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
    "        row = idx // 2 + 1\n",
    "        col = idx % 2 + 1\n",
    "\n",
    "        X_plot = X[[feat1, feat2], :]\n",
    "\n",
    "        # Create mesh grid\n",
    "        x_min, x_max = X_plot[0].min() - 0.5, X_plot[0].max() + 0.5\n",
    "        y_min, y_max = X_plot[1].min() - 0.5, X_plot[1].max() + 0.5\n",
    "\n",
    "        # Create mesh grid points\n",
    "        x_mesh = np.linspace(x_min, x_max, 100)\n",
    "        y_mesh = np.linspace(y_min, y_max, 100)\n",
    "        xx, yy = np.meshgrid(x_mesh, y_mesh)\n",
    "\n",
    "        # Prepare input for prediction\n",
    "        mesh_points = np.vstack([xx.ravel(), yy.ravel()])\n",
    "        X_pred = np.tile(mean_features.reshape(-1, 1), (1, mesh_points.shape[1]))\n",
    "        X_pred[feat1] = mesh_points[0]\n",
    "        X_pred[feat2] = mesh_points[1]\n",
    "\n",
    "        # Get predictions\n",
    "        Z_probs = model.predict(X_pred, return_probability=True)\n",
    "        Z = np.argmax(Z_probs, axis=0)\n",
    "        Z = Z.reshape(100, 100)\n",
    "\n",
    "        # Plot decision boundary\n",
    "        contour = fig.add_trace(\n",
    "            go.Contour(\n",
    "                x=x_mesh,\n",
    "                y=y_mesh,\n",
    "                z=Z,\n",
    "                colorscale=[\n",
    "                    [0, \"rgb(255,0,0)\"],  # red for setosa\n",
    "                    [0.5, \"rgb(0,0,255)\"],  # blue for versicolor\n",
    "                    [1, \"rgb(0,255,0)\"],  # green for virginica\n",
    "                ],\n",
    "                opacity=0.4,\n",
    "                colorbar=dict(\n",
    "                    title=\"Predicted Class\",\n",
    "                    ticktext=class_names,\n",
    "                    tickvals=[0, 1, 2],\n",
    "                    tickmode=\"array\",\n",
    "                    len=1.0,\n",
    "                    yanchor=\"top\",\n",
    "                    y=1,\n",
    "                )\n",
    "                if idx == 0\n",
    "                else None,\n",
    "                contours=dict(\n",
    "                    showlabels=True,\n",
    "                ),\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        # Plot training points\n",
    "        for i in range(3):\n",
    "            mask = y_classes == i\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=X_plot[0, mask],\n",
    "                    y=X_plot[1, mask],\n",
    "                    mode=\"markers\",\n",
    "                    name=class_names[i],\n",
    "                    marker=dict(\n",
    "                        size=8,\n",
    "                        color=colors[i],\n",
    "                    ),\n",
    "                    showlegend=True if idx == 0 else False,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Decision Boundaries for All Feature Pairs\",\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            title=\"Actual Classes\", yanchor=\"top\", y=0.99, xanchor=\"left\", x=1.05\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Update axes labels\n",
    "    for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
    "        row = idx // 2 + 1\n",
    "        col = idx % 2 + 1\n",
    "        fig.update_xaxes(title_text=feature_names[feat1], row=row, col=col)\n",
    "        fig.update_yaxes(title_text=feature_names[feat2], row=row, col=col)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def softmax_regression_example():\n",
    "    data = sns.load_dataset(\"iris\")\n",
    "    np.random.seed(45)\n",
    "    # one hot encode the y values\n",
    "    data = pd.get_dummies(data, columns=[\"species\"]).astype(float)\n",
    "    train_data = data.sample(frac=0.8)\n",
    "    test_data = data.drop(train_data.index)\n",
    "    X_train = train_data[[\"sepal_length\", \"sepal_width\"]].to_numpy().T\n",
    "    y_train = (\n",
    "        train_data[[\"species_setosa\", \"species_versicolor\", \"species_virginica\"]]\n",
    "        .to_numpy()\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    X_test = test_data[[\"sepal_length\", \"sepal_width\"]].to_numpy().T\n",
    "    y_test = (\n",
    "        test_data[[\"species_setosa\", \"species_versicolor\", \"species_virginica\"]]\n",
    "        .to_numpy()\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    layers = [2, 15, 5, 3]\n",
    "    nn = NeuralNetwork(\n",
    "        layers,\n",
    "        model_id=\"Softmax regression\",\n",
    "        layer_activations={1: \"relu\", 2: \"relu\", 3: \"softmax\"},\n",
    "    )\n",
    "    nn.initialise_weights()\n",
    "    adam_optimizer = ADAMOptimizer(batch_size=128, learning_rate=0.01)\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    from time import perf_counter\n",
    "\n",
    "    start_time = perf_counter()\n",
    "    adam_optimizer.train(nn, X_train, y_train, n_epochs=500, plot_cost=True, fig=fig)\n",
    "    end_time = perf_counter()\n",
    "    time = end_time - start_time\n",
    "    print(f\"\\n Optimiser: {adam_optimizer}\")\n",
    "    print(f\"Training time: {time:.4f} seconds\")\n",
    "    # Get predictions\n",
    "    train_Y_pred = nn.predict(X_train)\n",
    "    test_Y_pred = nn.predict(X_test)\n",
    "\n",
    "    # Convert one-hot encoded targets to class indices\n",
    "    y_train_classes = np.argmax(y_train, axis=0)\n",
    "    y_test_classes = np.argmax(y_test, axis=0)\n",
    "\n",
    "    train_Y_pred_classes = np.argmax(train_Y_pred, axis=0)\n",
    "    test_Y_pred_classes = np.argmax(test_Y_pred, axis=0)\n",
    "\n",
    "    print(f\"train_Y_pred_classes: {train_Y_pred_classes}\")\n",
    "\n",
    "    # Calculate accuracies using class indices\n",
    "    train_accuracy = (train_Y_pred_classes == y_train_classes).mean()\n",
    "    test_accuracy = (test_Y_pred_classes == y_test_classes).mean()\n",
    "\n",
    "    print(f\"Accuracy on train: {train_accuracy}\")\n",
    "    print(f\"Accuracy on test: {test_accuracy}\")\n",
    "    print(f\"Accuracy on train: {train_accuracy}\")\n",
    "    print(f\"Accuracy on test: {test_accuracy}\")\n",
    "\n",
    "    # plot the decision boundary\n",
    "    plot_decision_boundary(nn, X_train, y_train)\n",
    "\n",
    "\n",
    "softmax_regression_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebb037",
   "metadata": {},
   "source": [
    "### Batch Normalisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47434586",
   "metadata": {},
   "source": [
    "\n",
    "- Batch normalisation is a technique that normalises the inputs of each layer. The parameters $\\gamma$ and $\\beta$ are learnable parameters that scale and shift the normalised inputs. the values of $\\mu$ and $\\sigma^2$ are calculated for each mini-batch.\n",
    "$$\n",
    " \\begin{align*}\n",
    " \\mu &= \\frac{1}{m} \\sum_{i=1}^{m} z^{(i)} \\\\\n",
    " \\sigma^2 &= \\frac{1}{m} \\sum_{i=1}^{m} (z^{(i)} - \\mu)^2 \\\\\n",
    " z^{(i)}_{norm} &= \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    " \\tilde{z}^{(i)} &= \\gamma z^{(i)}_{norm} + \\beta\n",
    " \\end{align*}\n",
    " $$\n",
    " - We also keep track of the exponentially weighted average of the mean and variance to use during prediction.\n",
    " $$\n",
    " \\begin{align*}\n",
    " \\mu &= \\beta_1 \\mu_{t-1} + (1 - \\beta_1) \\mu_t \\\\\n",
    " \\sigma^2 &= \\beta_2 \\sigma_{t-1}^2 + (1 - \\beta_2) \\sigma_t^2\n",
    " \\end{align*}\n",
    " $$\n",
    " - Now the - The forward pass is calculated as\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]}+b^{[l]} \\\\\n",
    "\\mu^{[l]} &= Z^{(i)}.mean(axis=1) \\\\\n",
    "{\\sigma^{[l]}}^2 &= (z^{(i)} - \\mu)^2.mean(axis=1) \\\\\n",
    "Z^{(i)}_{norm} &= \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\\\\n",
    "A^{[l]} &= g(Z^{(i)}_{norm}), \\quad\\text{lth layer, g - activation function} \\\\ \n",
    "Z^{[0]} &= X \\\\\n",
    "\\hat{Y} &= A^{[L]} \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc7f8c",
   "metadata": {},
   "source": [
    "\n",
    "See `neural_network_with_bn(.md|.py)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa545583",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2214538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success! TensorFlow has found your Mac's GPU:\n",
      "- PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# This command lists all available physical devices (CPUs and GPUs)\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    print(f\"âœ… Success! TensorFlow has found your Mac's GPU:\")\n",
    "    for device in gpu_devices:\n",
    "        print(f\"- {device}\")\n",
    "else:\n",
    "    print(\"âŒ TensorFlow did not find the GPU. It will run on the CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab539b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 09:26:30.402362: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-10-01 09:26:30.402395: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-10-01 09:26:30.402400: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-10-01 09:26:30.402430: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-01 09:26:30.402439: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 iterations, x = 2.5024521350860596\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def simple_tensor_flow_example():\n",
    "    # in this example we will minimise x**2 - 5x + 6\n",
    "\n",
    "    x = tf.Variable(0.0, name=\"x\", dtype=tf.float32)\n",
    "\n",
    "    def loss():\n",
    "        return x**2 - 5 * x + 6\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    n = 100\n",
    "    for _ in range(n):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss()\n",
    "        gradients = tape.gradient(loss_value, [x])\n",
    "        optimizer.apply_gradients(zip(gradients, [x]))\n",
    "\n",
    "    print(f\"After {n} iterations, x = {x.numpy()}\")\n",
    "\n",
    "\n",
    "simple_tensor_flow_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-TpHDo9Oc-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
