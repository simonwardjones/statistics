{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5f2bd7-6b62-4a6b-8a00-469e4889a63a",
   "metadata": {},
   "source": [
    "# Deep Learning Specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a532ec-35f9-468d-8fa5-0df23e6717b5",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning, regulatrisation and optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e3ad3-362f-491f-854c-7575f70037d5",
   "metadata": {},
   "source": [
    "### L2 regularisation, dropout, vanishing and exploding gradients, stadard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755daf99-38d2-41dd-8ab1-cedce49ac7b0",
   "metadata": {},
   "source": [
    "- Decrease bias by making the model more complex, e.g. bigger network\n",
    "- Decrease variance with more data, regularisation, dropout, new architecture\n",
    "- The most common form of regularisation is L2 regularisation, so the cost function is\n",
    "$$\n",
    "\\begin{align}\n",
    "J &= -\\frac{1}{m} \\sum_{i=1}^my \\log(\\hat{y}^{i}) + (1-y^{i})\\log(1-\\hat{y}^{i}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L}\\|W^l\\|^2 \\\\\n",
    "J &= -\\frac{1}{m} \\sum_{i=1}^my \\log(\\hat{y}^{i}) + (1-y^{i})\\log(1-\\hat{y}^{i}) + \\frac{\\lambda}{2m} \n",
    " \\sum_{l=1}^{L}\\sum_{i=1}^{n_{l}}\\sum_{j=1}^{n_{l-1}}(W_{ij}^{[l]})^2 \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T) + \\frac{\\lambda}{2m}\\text{trace}(W^TW)\n",
    "\\end{align}\n",
    "$$\n",
    "- In dropout each node is dropped out based on parameter `keep_prob`. When implementing, adfter dropout, each activation vector $a_l$ is divided by the keep_prob to keep the scale correct whilst dropping out node - this is called inverted dropout\n",
    "- Normalise inputs using strandard scaling to increase the speed of training a nueral network\n",
    "- Vanishing and exploding gradients happend due to many multiplications of terms in back prop. To avoid this use good initialisation with $W^{[l]} = np.random.rand(shape) * \\sqrt(\\frac{2}{n^{[l-1]}})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff965a0c-88e4-4c45-805f-c17e1e9bd9c5",
   "metadata": {},
   "source": [
    "### L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2aaf1d4-e0c3-4526-ad32-f070b67d8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def leaky_relu(x, leaky_constant:float = 0.01):\n",
    "    return np.where(x > 0.0, x,  x * leaky_constantÃ)\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "\n",
    "def log_loss(A, Y, Ws, regularisation_lambda:float = 0):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    cost = - (1/m) * ((Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T))\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(np.square(W).sum() for W in Ws.values())\n",
    "\n",
    "def square_loss(A, Y, Ws, regularisation_lambda:float = 0):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    cost = - (1/m) * ((A - Y) @ (A - Y).T)\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(np.square(W).sum() for W in Ws.values())\n",
    "\n",
    "ACTIVATION_FUNCTIONS : dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"log_loss\": log_loss,\n",
    "    \"square_loss\": square_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909d42a1-15f9-4154-a98e-18e96173600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0,\n",
    "        cost_function: str = \"log_loss\"\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As = {}, {0:X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "        if cache:\n",
    "            self.Zs, self.As = Zs, As\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1.0 / m) * dZs[l] @ self.As[l-1].T\n",
    "            if self.regularisation_lambda:\n",
    "                dWs[l] += ((self.regularisation_lambda / m) * self.Ws[l])\n",
    "            dbs[l] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False, fig=None):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name=f'Cost {self.regularisation_lambda}')\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "            cost = self.cost(A, Y)\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                costs.append(cost.item()) # extract from (1, 1) array\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs\n",
    "                        fig.data[-1].y = costs\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eccb5ee1-2dc9-496e-88ed-53950c4be854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bde13428b84245afecc2e22aa749b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train with no regularisation: 0.8328651685393258\n",
      "Accuracy on test with no regularisation: 0.8100558659217877\n",
      "Accuracy on train with regularisation: 0.8314606741573034\n",
      "Accuracy on test with regularisation: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "def regularisation_example():\n",
    "\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    # Define a simple neural network architecture\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    # Fit with lambda = 0 (no regularization)\n",
    "    nn_no_reg = NeuralNetwork(layers, regularisation_lambda=0)\n",
    "    nn_no_reg.initialise_weights()\n",
    "    nn_no_reg.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_no_reg = nn_no_reg.predict(X_train)\n",
    "    test_Y_pred_no_reg = nn_no_reg.predict(X_test)\n",
    "\n",
    "    # Fit with lambda = 0.2 (with regularization)\n",
    "    nn_reg = NeuralNetwork(layers, regularisation_lambda=1)\n",
    "    nn_reg.initialise_weights()\n",
    "    nn_reg.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_reg = nn_reg.predict(X_train)\n",
    "    test_Y_pred_reg = nn_reg.predict(X_test)\n",
    "\n",
    "    train_accuracy_no_reg = (train_Y_pred_no_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_no_reg = (test_Y_pred_no_reg == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_reg = (train_Y_pred_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_reg = (test_Y_pred_reg == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train with no regularisation: {train_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on test with no regularisation: {test_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on train with regularisation: {train_accuracy_reg}\")\n",
    "    print(f\"Accuracy on test with regularisation: {test_accuracy_reg}\")\n",
    "\n",
    "\n",
    "regularisation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc7504-902b-4ae7-8f42-8a25dd11c6eb",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "930dc3fd-f069-4f6c-be65-2a21dd0eb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0:X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(int) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                # dj/dz = dj/da * da/dz\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * self.Ds[l] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1.0 / m) * dZs[l] @ self.As[l-1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                dWs[l] += ((self.regularisation_lambda / m) * self.Ws[l])\n",
    "            dbs[l] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False, fig=None):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name=self.model_id)\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "            cost = self.cost(A, Y).item()\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                if np.isnan(cost):\n",
    "                    cost = 10\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs\n",
    "                        fig.data[-1].y = costs\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dd565f1-7892-493c-aa40-8047a3d10b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8db12500a94852ab42b687273bb0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train with no regularisation: 0.8370786516853933\n",
      "Accuracy on test with no regularisation: 0.8044692737430168\n",
      "Accuracy on train with regularisation: 0.827247191011236\n",
      "Accuracy on test with regularisation: 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "def dropout_example():\n",
    "\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    # Define a simple neural network architecture\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    # Fit with lambda = 0 (no regularization)\n",
    "    nn_no_reg = NeuralNetwork(layers, keep_prob=1, model_id=\"No dropout\")\n",
    "    nn_no_reg.initialise_weights()\n",
    "    nn_no_reg.train(X_train, y_train, n_epochs=500, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_no_reg = nn_no_reg.predict(X_train)\n",
    "    test_Y_pred_no_reg = nn_no_reg.predict(X_test)\n",
    "\n",
    "    # Fit with lambda = 0.2 (with regularization)\n",
    "    nn_reg = NeuralNetwork(layers, keep_prob=0.8, model_id=\"20% drop out\")\n",
    "    nn_reg.initialise_weights()\n",
    "    nn_reg.train(X_train, y_train, n_epochs=500, log_every=50, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_reg = nn_reg.predict(X_train)\n",
    "    test_Y_pred_reg = nn_reg.predict(X_test)\n",
    "\n",
    "    train_accuracy_no_reg = (train_Y_pred_no_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_no_reg = (test_Y_pred_no_reg == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_reg = (train_Y_pred_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_reg = (test_Y_pred_reg == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train with no regularisation: {train_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on test with no regularisation: {test_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on train with regularisation: {train_accuracy_reg}\")\n",
    "    print(f\"Accuracy on test with regularisation: {test_accuracy_reg}\")\n",
    "\n",
    "\n",
    "dropout_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055148f-4770-4074-8549-680b3a1efba0",
   "metadata": {},
   "source": [
    "### Standard scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4423e2-0ade-4c00-9a53-bda96bbde2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35082f92ba9b451cbba56d5f5c88b561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.8356741573033708\n",
      "Accuracy on test: 0.8100558659217877\n",
      "Accuracy on train with scaled data: 0.8455056179775281\n",
      "Accuracy on test with scaled data: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.means: Optional[NDArray] = None\n",
    "        self.stds: Optional[NDArray] = None\n",
    "        self.is_fitted: bool = False\n",
    "\n",
    "    def fit_transform(self, X) -> NDArray:\n",
    "        self.means = X.mean(axis=1, keepdims=True)\n",
    "        self.stds = X.std(axis=1, keepdims=True)\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "    def transform(self, X) -> NDArray:\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "    def fit(self, X: NDArray) -> None:\n",
    "        self.means = X.mean(axis=1, keepdims=True)\n",
    "        self.stds = X.std(axis=1, keepdims=True)\n",
    "        self.is_fitted = True\n",
    "\n",
    "def standard_scaling_example():\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    nn = NeuralNetwork(layers, model_id=\"unscaled data\")\n",
    "    nn_scaled = NeuralNetwork(layers, model_id=\"scaled data\")\n",
    "    nn.initialise_weights()\n",
    "    nn_scaled.initialise_weights()\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "    nn.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    nn_scaled.train(X_train_scaled, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "\n",
    "    train_Y_pred = nn.predict(X_train)\n",
    "    test_Y_pred = nn.predict(X_test)\n",
    "    train_Y_pred_scaled = nn_scaled.predict(X_train_scaled)\n",
    "    test_Y_pred_scaled = nn_scaled.predict(X_test_scaled)\n",
    "\n",
    "    train_accuracy = (train_Y_pred == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy = (test_Y_pred == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_scaled = (train_Y_pred_scaled == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_scaled = (test_Y_pred_scaled == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train: {train_accuracy}\")\n",
    "    print(f\"Accuracy on test: {test_accuracy}\")\n",
    "    print(f\"Accuracy on train with scaled data: {train_accuracy_scaled}\")\n",
    "    print(f\"Accuracy on test with scaled data: {test_accuracy_scaled}\")\n",
    "\n",
    "standard_scaling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ef4da-66b2-4b89-b0f7-719e2ede8fe8",
   "metadata": {},
   "source": [
    "### Mini-batch training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e7d7d",
   "metadata": {},
   "source": [
    " - In mini-batch gradient descent, we use a subset of the training data to update the weights.\n",
    " - In batch gradient descent, we use all the training data to update the weights.\n",
    " - In stochastic gradient descent, we use a single training example to update the weights.\n",
    " - The momentum optimisation algorithm uses a moving average of the gradients to update the weights. Momentum helps by enabling optimizers to overcome local minima and saddle points, reduce oscillations in narrow valleys for a smoother path, and speed up learning by maintaining a consistent direction through flat or consistent gradient regions.\n",
    " $$\n",
    " \\begin{align*}\n",
    " v_t &= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla J(w) \\\\\n",
    " w &= w - \\alpha v_t\n",
    " \\end{align*}\n",
    " $$ where $v_t$ is the moving average of the gradients, $\\beta_1$ is the momentum coefficient, $\\nabla J(w)$ is the gradient of the cost function $J(w)$ with respect to the weights $w$, and $\\alpha$ is the learning rate.\n",
    " - The RMSprop optimisation algorithm maintains a moving average of the squared gradients, it then updates the weights using the gradients divided by the square root of the moving average. RMSprop adaptively adjusts learning rates: it increases them for parameters with small gradients (in flat areas) and decreases them for those with large gradients (in steep or oscillating areas), balancing updates. This method also normalizes gradient magnitudes, preventing them from becoming too small (vanishing) or too large (exploding), which leads to more stable and efficient training.\n",
    " $$\n",
    " \\begin{align*}\n",
    " s_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(w))^2 \\\\\n",
    " w &= w - \\alpha \\frac{\\nabla J(w)}{\\sqrt{s_t + \\epsilon}}\n",
    " \\end{align*}\n",
    " $$ where $s_t$ is the moving average of the squared gradients, $\\beta_2$ is the RMSprop coefficient, $\\nabla J(w)$ is the gradient of the cost function $J(w)$ with respect to the weights $w$, and $\\alpha$ is the learning rate.\n",
    " - The adam algorihtm combines the momentum and RMSprop optimisation algorithms. It uses a moving average of the gradients scaled by a moving average of the squared gradients. Adam also uses a bias correction term to adjust the moving averages.\n",
    " $$\n",
    " \\begin{align*}\n",
    " v_t &= \\beta_1 v_{t-1} + (1 - \\beta_1) \\nabla J(w) \\\\\n",
    " s_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla J(w))^2 \\\\\n",
    " \\hat{v_t} &= \\frac{v_t}{1 - \\beta_1^t} \\\\\n",
    " \\hat{s_t} &= \\frac{s_t}{1 - \\beta_2^t} \\\\\n",
    " w &= w - \\alpha \\frac{\\hat{v_t}}{\\sqrt{\\hat{s_t} + \\epsilon}}\n",
    " \\end{align*}\n",
    " $$ where $v_t$ is the moving average of the gradients, $s_t$ is the moving average of the squared gradients, $\\beta_1$ and $\\beta_2$ are the momentum and RMSprop coefficients, $\\nabla J(w)$ is the gradient of the cost function $J(w)$ with respect to the weights $w$, and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849c0b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b79d59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfde2e568dbb4ae295a5723fa273779a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: Gradient Descent\n",
      "Gradient Descent training time: 9.0928 seconds\n",
      "Accuracy on train: 0.8328651685393258\n",
      "Accuracy on test: 0.8100558659217877\n",
      "\n",
      " Optimiser: Mini-batch\n",
      "Mini-batch training time: 13.5394 seconds\n",
      "Accuracy on train: 0.8384831460674157\n",
      "Accuracy on test: 0.8100558659217877\n",
      "\n",
      " Optimiser: Momentum\n",
      "Momentum training time: 10.1611 seconds\n",
      "Accuracy on train: 0.8370786516853933\n",
      "Accuracy on test: 0.8100558659217877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_18614/328717897.py:28: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_18614/328717897.py:28: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in matmul\n",
      "\n",
      "/Users/simonward-jones/Library/Caches/pypoetry/virtualenvs/statisitics-PieJMifL-py3.10/lib/python3.10/site-packages/jupyter_client/session.py:721: UserWarning:\n",
      "\n",
      "Message serialization failed with:\n",
      "Out of range float values are not JSON compliant\n",
      "Supporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: RMSProp\n",
      "RMSProp training time: 15.1471 seconds\n",
      "Accuracy on train: 0.9213483146067416\n",
      "Accuracy on test: 0.8212290502793296\n",
      "\n",
      " Optimiser: ADAM\n",
      "ADAM training time: 13.0351 seconds\n",
      "Accuracy on train: 0.9283707865168539\n",
      "Accuracy on test: 0.8044692737430168\n",
      "\n",
      " Optimiser: ADAM with learning rate decay\n",
      "ADAM with learning rate decay training time: 12.5911 seconds\n",
      "Accuracy on train: 0.8469101123595506\n",
      "Accuracy on test: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l: \"sigmoid\" for l in range(1, self.L)} | {\n",
    "                self.L: \"sigmoid\"\n",
    "            }\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.params = {}\n",
    "        for l, (n_l, n_l_minus_1) in enumerate(\n",
    "            zip(self.layer_sizes[1:], self.layer_sizes), start=1\n",
    "        ):\n",
    "            self.params[f\"W{l}\"] = np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(\n",
    "                2 / n_l_minus_1\n",
    "            )\n",
    "            self.params[f\"b{l}\"] = np.zeros((n_l, 1))\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.params=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0: X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            W = self.params[f\"W{l}\"]\n",
    "            b = self.params[f\"b{l}\"]\n",
    "            Zs[l] = W @ As[l - 1] + b\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out but not on the output layer\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(\n",
    "                    int\n",
    "                ) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y):\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        grads = {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                W_l_plus_1 = self.params[f\"W{l+1}\"]\n",
    "                dZs[l] = (\n",
    "                    W_l_plus_1.T\n",
    "                    @ dZs[l + 1]\n",
    "                    * self.Ds[l]\n",
    "                    * ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](\n",
    "                        self.Zs[l]\n",
    "                    )\n",
    "                )\n",
    "            grads[f\"dW{l}\"] = (1.0 / m) * dZs[l] @ self.As[l - 1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                grads[f\"dW{l}\"] += (self.regularisation_lambda / m) * self.params[\n",
    "                    f\"W{l}\"\n",
    "                ]\n",
    "            grads[f\"db{l}\"] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def cost(self, A, Y) -> float:\n",
    "        # For cost, we need to pass the weights. We'll extract them from self.params.\n",
    "        Ws = {l: self.params[f\"W{l}\"] for l in range(1, self.L + 1)}\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Incorrect value for self.cost_function:= {self.cost_function}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"This implements gradient descent. With optional mini-batching.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.1, batch_size: int | None = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        \"\"\"Update model parameters using grads returned from backward.\"\"\"\n",
    "        for param_key in model.params:\n",
    "            model.params[param_key] -= self.learning_rate * grads[f\"d{param_key}\"]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        n_epochs=1000,\n",
    "        log_every: int | None = None,\n",
    "        plot_cost=False,\n",
    "        fig=None,\n",
    "        plot_every=10,\n",
    "    ):\n",
    "        costs, epochs = [], []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode=\"lines+markers\", name=model.model_id)\n",
    "            fig.update_layout(\n",
    "                title=\"Training Cost over Epochs\",\n",
    "                xaxis_title=\"Epoch\",\n",
    "                yaxis_title=\"Cost\",\n",
    "            )\n",
    "\n",
    "        m = X.shape[1]\n",
    "        batch_size = self.batch_size if self.batch_size is not None else m\n",
    "\n",
    "        training_iteration = 1\n",
    "        for epoch in range(n_epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X[:, i : i + batch_size]\n",
    "                Y_batch = Y[:, i : i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                A = model.forward(X_batch, cache=True)\n",
    "                grads = model.backward(Y_batch)\n",
    "                self.update_model_params(model, grads, training_iteration)\n",
    "                training_iteration += 1\n",
    "\n",
    "            # Compute cost on the whole dataset after epoch\n",
    "            A_full = model.forward(X, cache=False)\n",
    "            cost = model.cost(A_full, Y)\n",
    "            if log_every and epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost and fig is not None:\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch + 1)\n",
    "                if epoch % plot_every == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs  # type: ignore\n",
    "                        fig.data[-1].y = costs  # type: ignore\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    \"\"\"Implements momentum optimizer.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta * v_{t-1} + grad_t\n",
    "    param_t = param_{t-1} - learning_rate * v_t\n",
    "\n",
    "    Where:\n",
    "    - S_t is the second moment of the gradient\n",
    "    - param_t is the parameter\n",
    "    - grad_t is the gradient of the cost function with respect to the parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.cache:\n",
    "                self.cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.cache[param_key] = (\n",
    "                self.beta * self.cache[param_key]\n",
    "                + (1 - self.beta) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            model.params[param_key] -= self.learning_rate * self.cache[param_key]\n",
    "\n",
    "\n",
    "class RMSPropOptimizer(Optimizer):\n",
    "    \"\"\"Implements RMSProp optimizer.\n",
    "\n",
    "    Here we track the exponentially weighted average of the squared gradients (second moment).\n",
    "\n",
    "    The update rule is:\n",
    "    S_t = beta * S_{t-1} + (1 - beta) * (grad_t)^2\n",
    "    param_t = param_{t-1} - learning_rate * grad_t / (sqrt(S_t) + epsilon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        # Standard RMSProp does NOT use bias correction (unlike Adam)\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.s_cache:\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.s_cache[param_key] = (\n",
    "                self.beta * self.s_cache[param_key]\n",
    "                + (1 - self.beta) * (grads[f\"d{param_key}\"] ** 2)\n",
    "            )\n",
    "            model.params[param_key] -= (\n",
    "                self.learning_rate\n",
    "                * grads[f\"d{param_key}\"]\n",
    "                / (np.sqrt(self.s_cache[param_key]) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "class ADAMOptimizer(Optimizer):\n",
    "    \"\"\"Implements ADAM optimizer with learning rate decay.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta_1 * v_{t-1} + (1 - beta_1) * grad\n",
    "    s_t = beta_2 * s_{t-1} + (1 - beta_2) * (grad_t)^2\n",
    "    v_t_corrected = v_t / (1 - beta_1^t)\n",
    "    s_t_corrected = s_t / (1 - beta_2^t)\n",
    "    param_t = param_{t-1} - learning_rate * v_t_corrected / (sqrt(s_t_corrected) + epsilon)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.999,\n",
    "        epsilon: float = 1e-8,\n",
    "        learning_rate_decay: float = 0,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta_1 = beta_1  # for the momentum\n",
    "        self.beta_2 = beta_2  # for the second moment (RMSProp)\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.v_cache = {}\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        learning_rate = self.learning_rate * (\n",
    "            1 / (1 + self.learning_rate_decay * training_iteration)\n",
    "        )\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.v_cache:\n",
    "                self.v_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.v_cache[param_key] = (\n",
    "                self.beta_1 * self.v_cache[param_key]\n",
    "                + (1 - self.beta_1) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            self.s_cache[param_key] = (\n",
    "                self.beta_2 * self.s_cache[param_key]\n",
    "                + (1 - self.beta_2) * grads[f\"d{param_key}\"] ** 2\n",
    "            )\n",
    "            v_t_corrected = self.v_cache[param_key] / (1 - self.beta_1 ** training_iteration)\n",
    "            s_t_corrected = self.s_cache[param_key] / (1 - self.beta_2 ** training_iteration)\n",
    "\n",
    "            model.params[param_key] -= (\n",
    "                learning_rate * v_t_corrected / (np.sqrt(s_t_corrected) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "def optimizer_example():\n",
    "    X_train = pd.read_feather(\"../titanic/processed/X_train.feather\").to_numpy().T\n",
    "    y_train = pd.read_feather(\"../titanic/processed/y_train.feather\").to_numpy().T\n",
    "    X_test = pd.read_feather(\"../titanic/processed/X_test.feather\").to_numpy().T\n",
    "    y_test = pd.read_feather(\"../titanic/processed/y_test.feather\").to_numpy().T\n",
    "\n",
    "    layers = [30, 50, 20, 1]\n",
    "    nn = NeuralNetwork(layers, model_id=\"Using an optimizer\")\n",
    "    nn_mini_batch = NeuralNetwork(layers, model_id=\"Using a mini-batch optimizer\")\n",
    "    nn_momentum = NeuralNetwork(layers, model_id=\"Using a momentum optimizer\")\n",
    "    nn_rmsprop = NeuralNetwork(layers, model_id=\"Using a RMSProp optimizer\")\n",
    "    nn_adam = NeuralNetwork(layers, model_id=\"Using a ADAM optimizer\")\n",
    "    nn_adam_decay = NeuralNetwork(layers, model_id=\"Using a ADAM optimizer with learning rate decay\")\n",
    "\n",
    "    nn.initialise_weights()\n",
    "    nn_mini_batch.initialise_weights()\n",
    "    nn_momentum.initialise_weights()\n",
    "    nn_rmsprop.initialise_weights()\n",
    "    nn_adam.initialise_weights()\n",
    "    nn_adam_decay.initialise_weights()\n",
    "\n",
    "    optimizer = Optimizer(learning_rate=0.5)\n",
    "    mini_batch_optimizer = Optimizer(batch_size=128)\n",
    "    momentum_optimizer = MomentumOptimizer(batch_size=128)\n",
    "    rmsprop_optimizer = RMSPropOptimizer(batch_size=128)\n",
    "    adam_optimizer = ADAMOptimizer(batch_size=128 )\n",
    "    adam_decay_optimizer = ADAMOptimizer(batch_size=128, learning_rate_decay=0.5)\n",
    "\n",
    "    models = [nn, nn_mini_batch, nn_momentum, nn_rmsprop, nn_adam, nn_adam_decay]\n",
    "    optimizers = [optimizer, mini_batch_optimizer, momentum_optimizer, rmsprop_optimizer, adam_optimizer, adam_decay_optimizer]\n",
    "    names = [\"Gradient Descent\", \"Mini-batch\", \"Momentum\", \"RMSProp\", \"ADAM\", \"ADAM with learning rate decay\"]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    from time import perf_counter\n",
    "\n",
    "    for model, optimizer, name in zip(models, optimizers, names):\n",
    "        start_time = perf_counter()\n",
    "        optimizer.train(model, X_train, y_train, n_epochs=500, plot_cost=True, fig=fig)\n",
    "        end_time = perf_counter()\n",
    "        time = end_time - start_time\n",
    "        print(f\"\\n Optimiser: {name}\")\n",
    "        print(f\"{name} training time: {time:.4f} seconds\")\n",
    "        train_Y_pred = model.predict(X_train)\n",
    "        test_Y_pred = model.predict(X_test)\n",
    "        train_accuracy = (train_Y_pred == y_train).sum() / y_train.shape[1]\n",
    "        test_accuracy = (test_Y_pred == y_test).sum() / y_test.shape[1]\n",
    "        print(f\"Accuracy on train: {train_accuracy}\")\n",
    "        print(f\"Accuracy on test: {test_accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cd2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
