{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbc03ff-580c-49e7-b1b3-c0bee2e03c64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Neural Networks and Deep Learning \n",
    "### Course 1 of Deep learning Specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e4845",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### House Price prediction\n",
    "\n",
    "![House Price prediction](./img/motivating-example.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a464e2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scale driving performance\n",
    "\n",
    "![Scale driving performance](./img/scale-driving-performance.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdf5d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training data notation\n",
    "\n",
    "![Training data notation](./img/training-data-notation.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a667c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Logistic regression\n",
    "\n",
    "![Logistic regression](./img/logistic-regression.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d89b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural network\n",
    "\n",
    "![Neural network](./img/nn.drawio.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e4a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def leaky_relu(x, leaky_constant:float = 0.01):\n",
    "    return np.where(x > 0.0, x,  x * leaky_constant)\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "def log_loss(A, Y):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    return - (1/m) * (Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T)\n",
    "\n",
    "def square_loss(A, Y):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    return - (1/m) * (A - Y) @ (A - Y).T\n",
    "\n",
    "ACTIVATION_FUNCTIONS : dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"log_loss\": log_loss,\n",
    "    \"square_loss\": square_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09c27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        cost_function: str = \"log_loss\"\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate\n",
    "        self.layer_activations = layer_activations\n",
    "        if not layer_activations:\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # Note layer indexes are off by one because python indexes by 0\n",
    "        # so Ws[0] is really W^{[1]}\n",
    "\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As = {}, {0:X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "        if cache:\n",
    "            self.Zs, self.As = Zs, As\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        # [w1, w2, w3]\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1 / m) * dZs[l] @ self.As[l-1].T\n",
    "            dbs[l] = (1 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            fig = go.FigureWidget()\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name='Cost')\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "            display(fig)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            cost = self.cost(A, Y)\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                costs.append(float(cost))\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[0].x = epochs\n",
    "                        fig.data[0].y = costs\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8ebd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Weights initialised\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4ae63617dc43a8ae811fb41c62de1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Cost',\n",
       "              'type': 'scatter',\n",
       "              'uid': '0ce1cee0-b776-4139-a483-7648ff80e29e',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training Cost over Epochs'},\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Cost'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cost after epoch 0 = [[0.82054377]]\n",
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_36412/1060446905.py:87: DeprecationWarning:\n",
      "\n",
      "Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "\n",
      "INFO:root:Cost after epoch 100 = [[0.48283017]]\n",
      "INFO:root:Cost after epoch 200 = [[0.43837833]]\n",
      "INFO:root:Cost after epoch 300 = [[0.42343525]]\n",
      "INFO:root:Cost after epoch 400 = [[0.41595906]]\n",
      "INFO:root:Cost after epoch 500 = [[0.41096105]]\n",
      "INFO:root:Cost after epoch 600 = [[0.4069837]]\n",
      "INFO:root:Cost after epoch 700 = [[0.40356924]]\n",
      "INFO:root:Cost after epoch 800 = [[0.40051146]]\n",
      "INFO:root:Cost after epoch 900 = [[0.39769467]]\n",
      "INFO:root:Cost after epoch 1000 = [[0.39504862]]\n",
      "INFO:root:Cost after epoch 1100 = [[0.39253103]]\n",
      "INFO:root:Cost after epoch 1200 = [[0.3901173]]\n",
      "INFO:root:Cost after epoch 1300 = [[0.38779262]]\n",
      "INFO:root:Cost after epoch 1400 = [[0.38554666]]\n",
      "INFO:root:Cost after epoch 1500 = [[0.38337106]]\n",
      "INFO:root:Cost after epoch 1600 = [[0.38125848]]\n",
      "INFO:root:Cost after epoch 1700 = [[0.37920222]]\n",
      "INFO:root:Cost after epoch 1800 = [[0.37719602]]\n",
      "INFO:root:Cost after epoch 1900 = [[0.37523366]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30, 712)\n",
      "layer_activations = {1: 'sigmoid', 2: 'sigmoid', 3: 'sigmoid'}\n",
      "L = 3\n",
      "ws shapes: [(1, (50, 30)), (2, (20, 50)), (3, (1, 20))]\n",
      "As shapes: [(0, (30, 712)), (1, (50, 712)), (2, (20, 712)), (3, (1, 712))]\n",
      "zs shapes: [(1, (50, 712)), (2, (20, 712)), (3, (1, 712))]\n",
      "Accuracy on test: 0.8324022346368715\n"
     ]
    }
   ],
   "source": [
    "def example():\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "    layer_sizes = [30, 50, 20, 1] # L = 3, A[3] = Yhat\n",
    "    neural_network = NeuralNetwork(layer_sizes=layer_sizes)\n",
    "\n",
    "    # uncomment to see relu converges better !\n",
    "    # layer_sizes = [30, 50, 1] # L = 3, A[3] = Yhat\n",
    "    # neural_network = NeuralNetwork(\n",
    "    #     layer_sizes=layer_sizes,\n",
    "    #     layer_activations={1:\"relu\", 2:\"relu\", 3:\"sigmoid\"},\n",
    "    # )\n",
    "\n",
    "    neural_network.initialise_weights()\n",
    "    neural_network.train(X_train, y_train, n_epochs=2000, plot_cost=True)\n",
    "\n",
    "    y_test_pred = neural_network.predict(X_test)\n",
    "    accuracy = (y_test_pred == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"layer_activations = {neural_network.layer_activations}\")\n",
    "    print(f\"L = {neural_network.L}\")\n",
    "    print(\"ws shapes:\",  [(i, w.shape) for i, w in neural_network.Ws.items()])\n",
    "    print(\"As shapes:\",  [(i, a.shape) for i, a in neural_network.As.items()])\n",
    "    print(\"zs shapes:\",  [(i, z.shape) for i, z in neural_network.Zs.items()])\n",
    "    print(f\"Accuracy on test: {accuracy}\")\n",
    "\n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487e281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
