{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5f2bd7-6b62-4a6b-8a00-469e4889a63a",
   "metadata": {},
   "source": [
    "# Deep Learning Specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3ccfb-5af2-4f31-9555-fd5b5ac70bd8",
   "metadata": {},
   "source": [
    "## Neural networks and deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef1b21d-4c74-4e09-8ac0-15addad093cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic regression as neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c781a-ef01-4fb5-be97-7330282e1ca6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Inputs are images of cats a pixels of rgb and output in 0 or 1\n",
    "- m training examples (x, y) where $x\\in\\mathbb{R}^n, y\\in\\{0,1\\}$\n",
    "- Often X represents the training data where the rows are samples however in this course the columns of X are the training examples and hence has shape (n, m).\n",
    "- Y represents the vector of y values with shape (m, 1)\n",
    "- $\\hat{y}$ is the prediction. In the cat example $\\hat{y} = P(y=1|x)$\n",
    "- In logisitic regression $\\hat{y}=\\sigma(w^Tx+b)$ where $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "- Loss function $L(\\hat{y}, y) = - (y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$\n",
    "- Cost function $J(w, b) = \\frac{1}{m} \\sum_{i=1}^mL(\\hat{y}^i, y^i)$, J is convex\n",
    "- Gradient descent updates w and b to minimise the cost\n",
    "- We repeatedly update $w := w-\\alpha\\frac{\\partial J}{\\partial w} = w-\\alpha dw$ where alpha is the learning rate and  $b := b-\\alpha\\frac{\\partial J}{\\partial b} = b -\\alpha db$\n",
    "- When implementing we need to vectorise the implementation\n",
    "- Logistic regression diagram\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x1(($$x_1$$)) --> z(\"$$z = w^Tx + b$$\")\n",
    "    x2(($$x_2$$)) --> z\n",
    "    x3(($$x_3$$)) --> z\n",
    "    z --> a(\"$$\\hat{y} = a= \\sigma(z) $$\")\n",
    "```\n",
    "- Logistic regression equations are often summarised as\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{(i)} &= w^Tx^{(i)}+b \\\\\n",
    "\\hat{y}^{(i)} &= a^{(i)} = sigmoid(z^{(i)}) \\\\\n",
    "L(a^{(i)}, y^{(i)}) &= -y^{(i)}\\log(a^{(i)}) - (1 - y^{(i)})\\log(1-a^{(i)}) \\\\\n",
    "J &= \\frac{1}{m}\\sum_{i=1}^mL(a^{(i)} - y^{(i)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In vector form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= \\sigma(W^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)}) \\\\\n",
    "\\frac{\\partial J}{\\partial W} &= dW = \\frac{1}{m}X(A-Y)^T \\\\\n",
    "\\frac{\\partial J}{\\partial b} &= db = \\frac{1}{m}\\sum_{i=1}^m (a^{(i)}-y^{(i)}) \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T)\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "X - dim: (n, m) \\quad \n",
    "w - dim: (n, 1) \\quad\n",
    "b - dim: (1, 1) \\quad\n",
    "A - dim: (1, m) \\quad\n",
    "Y - dim: (1, m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7302c-360c-4b98-83fe-b0620096e00e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shallow neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a054117-9a86-42e9-822d-148ca6975c18",
   "metadata": {},
   "source": [
    "- For a network we use super script [i] to descripe the ith layer where n the dimension of x in $n^{[0]}$\n",
    "- network flow\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x1(($$x_1$$)) --> z1(\"$$z^{[1]} = W^{[1]}x + b[1]$$\")\n",
    "    x2(($$x_2$$)) --> z1\n",
    "    x3(($$x_3$$)) --> z1\n",
    "    z1 --> a1(\"$$a^{[1]} = \\sigma(z^{[1]}) $$\")\n",
    "    a1 --> z2(\"$$z^{[2]} = W^{[2]}a^{[1]} + b[2]$$\")\n",
    "    z2 --> a2(\"$$\\hat{y} = a^{[2]} = \\sigma(z^{[2]}) $$\")\n",
    "```\n",
    "- The forward pass is calculated as\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{[1](i)} &= W^{[1]}x^{(i)}+b^{[1]}, \\quad\\text{i is the ith sample} \\\\\n",
    "z^{[1]} &= W^{[1]}X+b^{[1]} \\\\\n",
    "A^{[1]} &= \\sigma(z^{[1]}) \\\\ \n",
    "z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "Y = A^{[2]} &= \\sigma(z^{[2]}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- Activation functions could be tanh, relu, leaky relu or the signmoid\n",
    "- Taking the example of a two layer network (note we don't include the input layer in the count so we have 1 hidden layer) we have params\n",
    "\n",
    "$$\\begin{align}\n",
    "x^{(i)} &- dim: (n, 1) \\\\\n",
    "X &- dim: (n, m) \\\\\n",
    "W^{[1]} &- dim: (n^{[1]}, n^{[0]}) \\\\\n",
    "b^{[1]} &- dim: (n^{[1]}, 1) \\\\\n",
    "Z^{[1]} &- dim: (n^{[1]}, m) \\\\\n",
    "A^{[1]} &- dim: (n^{[1]}, m) \\\\\n",
    "W^{[2]} &- dim: (n^{[2]}, n^{[1]}) \\\\\n",
    "b^{[2]} &- dim: (n^{[2]}, 1) \\\\\n",
    "Z^{[2]} &- dim: (n^{[2]}, m) \\\\\n",
    "A^{[2]} &- dim: (n^{[2]}, m) \\\\\n",
    "&n^{[2]} = 1 \\\\\n",
    "Y &- dim: (n^{[2]}, m) \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "- Back prop equations\n",
    "$$\\begin{align}\n",
    "dZ^{[2]} &= A^{[2]} - Y \\\\\n",
    "dW^{[2]} &= \\frac{1}{m}dZ^{[2]}A^{[1]T} \\\\\n",
    "db^{[2]} &= \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) \\\\\n",
    "dZ^{[1]} &= W^{[2]T}dZ^{[2]} * \\sigma'(Z^{[1]}) \\\\\n",
    "dW^{[1]} &= \\frac{1}{m}dZ^{[1]}X^T \\\\\n",
    "db^{[1]} &= \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True) \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "- initialise weights randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb499d-a799-473c-8705-ccb491d6c96f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300d15f-e027-4911-836a-a40966a21c39",
   "metadata": {},
   "source": [
    " - The equations for the forward pass can we summarised as below where there are $L$ layers\n",
    " $$\n",
    "\\begin{align}\n",
    "z^{[l](i)} &= W^{[l]}a^{[l-1](i))}+b^{[l]}, \\quad\\text{ith sample, lth layer} \\\\\n",
    "a^{[l](i)} &= g(z^{[l](i)}) , \\quad\\text{ith sample, lth layer, g - activation function} \\\\\n",
    "z^{[0](i)} &= x^{(i)} \\\\\n",
    "\\hat{y^{(i)}} &= a^{[L](i)} \\\\\n",
    "L(a^{(i)}, y^{(i)}) &= -y^{(i)}\\log(a^{(i)}) - (1 - y^{(i)})\\log(1-a^{(i)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    " - As vectors\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]}+b^{[l]} \\\\\n",
    "A^{[l]} &= g(Z^{[l]}), \\quad\\text{lth layer, g - activation function} \\\\ \n",
    "Z^{[0]} &= X \\\\\n",
    "\\hat{Y} &= A^{[L]} \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T)\n",
    "\\end{align}\n",
    "$$\n",
    "- The equations for the backward pass\n",
    "$$\n",
    "\\begin{align}\n",
    "dZ^{[L]} &= A^{[L]} - Y \\\\\n",
    "dZ^{[l]} &= W^{[l+1]^T}dZ^{[l+1]} * g'(Z^{[l]}) \\quad l\\in\\{1...L-1\\} \\\\\n",
    "dW^{[l]} &= \\frac{1}{m}dZ^{[l]}A^{[l-1]^T} \\quad l\\in\\{1...L-1\\} \\\\\n",
    "db^{[l]} &= \\frac{1}{m}np.sum(dZ^{[l]}, axis=1, keepdims=True) \\quad l\\in\\{1...L-1\\} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fc21ff5-a271-47e2-8059-f6eeed76a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def leaky_relu(x, leaky_constant:float = 0.01):\n",
    "    return np.where(x > 0.0, x,  x * leaky_constantÏ)\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "def log_loss(A, Y):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    return - (1/m) * (Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T)\n",
    "\n",
    "def square_loss(A, Y):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    return - (1/m) * (A - Y) @ (A - Y).T\n",
    "\n",
    "ACTIVATION_FUNCTIONS : dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"log_loss\": log_loss,\n",
    "    \"square_loss\": square_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c195be8-b644-41b6-8cc2-e32973711ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        cost_function: str = \"log_loss\"\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate\n",
    "        self.layer_activations = layer_activations\n",
    "        if not layer_activations:\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # Note layer indexes are off by one because python indexes by 0\n",
    "        # so Ws[0] is really W^{[1]}\n",
    "\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As = {}, {0:X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "        if cache:\n",
    "            self.Zs, self.As = Zs, As\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        # [w1, w2, w3]\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1 / m) * dZs[l] @ self.As[l-1].T\n",
    "            dbs[l] = (1 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            fig = go.FigureWidget()\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name='Cost')\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "            display(fig)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            cost = self.cost(A, Y)\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                costs.append(float(cost))\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[0].x = epochs\n",
    "                        fig.data[0].y = costs\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5adc5b4c-8d56-4704-b3ca-9bbf99fa875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "    layer_sizes = [30, 50, 20, 1] # L = 3, A[3] = Yhat\n",
    "    neural_network = NeuralNetwork(layer_sizes=layer_sizes)\n",
    "\n",
    "    # uncomment to see relu converges better !\n",
    "    # layer_sizes = [30, 50, 1] # L = 3, A[3] = Yhat\n",
    "    # neural_network = NeuralNetwork(\n",
    "    #     layer_sizes=layer_sizes,\n",
    "    #     layer_activations={1:\"relu\", 2:\"relu\", 3:\"sigmoid\"},\n",
    "    # )\n",
    "\n",
    "    neural_network.initialise_weights()\n",
    "    neural_network.train(X_train, y_train, n_epochs=2000, plot_cost=True)\n",
    "\n",
    "    y_test_pred = neural_network.predict(X_test)\n",
    "    accuracy = (y_test_pred == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"layer_activations = {neural_network.layer_activations}\")\n",
    "    print(f\"L = {neural_network.L}\")\n",
    "    print(\"ws shapes:\",  [(i, w.shape) for i, w in neural_network.Ws.items()])\n",
    "    print(\"As shapes:\",  [(i, a.shape) for i, a in neural_network.As.items()])\n",
    "    print(\"zs shapes:\",  [(i, z.shape) for i, z in neural_network.Zs.items()])\n",
    "    print(f\"Accuracy on test: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da0f9dc-e0b2-4d71-a92b-70bdcbb7dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Weights initialised\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226cc12e05304c87b0aee79483dc4255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Cost',\n",
       "              'type': 'scatter',\n",
       "              'uid': '5083a0c6-8206-4719-a84e-679b0abface0',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training Cost over Epochs'},\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Cost'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cost after epoch 0 = [[0.67791741]]\n",
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_94230/1060446905.py:87: DeprecationWarning:\n",
      "\n",
      "Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "\n",
      "INFO:root:Cost after epoch 100 = [[0.46155934]]\n",
      "INFO:root:Cost after epoch 200 = [[0.43274977]]\n",
      "INFO:root:Cost after epoch 300 = [[0.42003758]]\n",
      "INFO:root:Cost after epoch 400 = [[0.41334262]]\n",
      "INFO:root:Cost after epoch 500 = [[0.40872809]]\n",
      "INFO:root:Cost after epoch 600 = [[0.40503333]]\n",
      "INFO:root:Cost after epoch 700 = [[0.40186904]]\n",
      "INFO:root:Cost after epoch 800 = [[0.39904907]]\n",
      "INFO:root:Cost after epoch 900 = [[0.39646523]]\n",
      "INFO:root:Cost after epoch 1000 = [[0.39405118]]\n",
      "INFO:root:Cost after epoch 1100 = [[0.39176588]]\n",
      "INFO:root:Cost after epoch 1200 = [[0.38958391]]\n",
      "INFO:root:Cost after epoch 1300 = [[0.3874885]]\n",
      "INFO:root:Cost after epoch 1400 = [[0.38546622]]\n",
      "INFO:root:Cost after epoch 1500 = [[0.38350336]]\n",
      "INFO:root:Cost after epoch 1600 = [[0.38158428]]\n",
      "INFO:root:Cost after epoch 1700 = [[0.37969143]]\n",
      "INFO:root:Cost after epoch 1800 = [[0.37780638]]\n",
      "INFO:root:Cost after epoch 1900 = [[0.37591085]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30, 712)\n",
      "layer_activations = {1: 'sigmoid', 2: 'sigmoid', 3: 'sigmoid'}\n",
      "L = 3\n",
      "ws shapes: [(1, (50, 30)), (2, (20, 50)), (3, (1, 20))]\n",
      "As shapes: [(0, (30, 712)), (1, (50, 712)), (2, (20, 712)), (3, (1, 712))]\n",
      "zs shapes: [(1, (50, 712)), (2, (20, 712)), (3, (1, 712))]\n",
      "Accuracy on test: 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a532ec-35f9-468d-8fa5-0df23e6717b5",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning, regulatrisation and optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e3ad3-362f-491f-854c-7575f70037d5",
   "metadata": {},
   "source": [
    "### L2 regularisation, dropout, vanishing and exploding gradients, stadard scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755daf99-38d2-41dd-8ab1-cedce49ac7b0",
   "metadata": {},
   "source": [
    "- Decrease bias by making the model more complex, e.g. bigger network\n",
    "- Decrease variance with more data, regularisation, dropout, new architecture\n",
    "- The most common form of regularisation is L2 regularisation, so the cost function is\n",
    "$$\n",
    "\\begin{align}\n",
    "J &= -\\frac{1}{m} \\sum_{i=1}^my \\log(\\hat{y}^{i}) + (1-y^{i})\\log(1-\\hat{y}^{i}) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L}\\|W^l\\|^2 \\\\\n",
    "J &= -\\frac{1}{m} \\sum_{i=1}^my \\log(\\hat{y}^{i}) + (1-y^{i})\\log(1-\\hat{y}^{i}) + \\frac{\\lambda}{2m} \n",
    " \\sum_{l=1}^{L}\\sum_{i=1}^{n_{l}}\\sum_{j=1}^{n_{l-1}}(W_{ij}^{[l]})^2 \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T) + \\frac{\\lambda}{2m}\\text{trace}(W^TW)\n",
    "\\end{align}\n",
    "$$\n",
    "- In dropout each node is dropped out based on parameter `keep_prob`. When implementing, adfter dropout, each activation vector $a_l$ is divided by the keep_prob to keep the scale correct whilst dropping out node - this is called inverted dropout\n",
    "- Normalise inputs using strandard scaling to increase the speed of training a nueral network\n",
    "- Vanishing and exploding gradients happend due to many multiplications of terms in back prop. To avoid this use good initialisation with $W^{[l]} = np.random.rand(shape) * \\sqrt(\\frac{2}{n^{[l-1]}})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff965a0c-88e4-4c45-805f-c17e1e9bd9c5",
   "metadata": {},
   "source": [
    "### L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2aaf1d4-e0c3-4526-ad32-f070b67d8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def leaky_relu(x, leaky_constant:float = 0.01):\n",
    "    return np.where(x > 0.0, x,  x * leaky_constantÏ)\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "\n",
    "def log_loss(A, Y, Ws, regularisation_lambda:float = 0):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    cost = - (1/m) * ((Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T))\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(np.square(W).sum() for W in Ws.values())\n",
    "\n",
    "def square_loss(A, Y, Ws, regularisation_lambda:float = 0):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    cost = - (1/m) * ((A - Y) @ (A - Y).T)\n",
    "    if not regularisation_lambda:\n",
    "        return cost\n",
    "    return cost + (regularisation_lambda / (2 * m)) * sum(np.square(W).sum() for W in Ws.values())\n",
    "\n",
    "ACTIVATION_FUNCTIONS : dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"log_loss\": log_loss,\n",
    "    \"square_loss\": square_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909d42a1-15f9-4154-a98e-18e96173600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0,\n",
    "        cost_function: str = \"log_loss\"\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As = {}, {0:X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "        if cache:\n",
    "            self.Zs, self.As = Zs, As\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1.0 / m) * dZs[l] @ self.As[l-1].T\n",
    "            if self.regularisation_lambda:\n",
    "                dWs[l] += ((self.regularisation_lambda / m) * self.Ws[l])\n",
    "            dbs[l] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False, fig=None):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name=f'Cost {self.regularisation_lambda}')\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "            cost = self.cost(A, Y)\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                costs.append(cost.item()) # extract from (1, 1) array\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs\n",
    "                        fig.data[-1].y = costs\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eccb5ee1-2dc9-496e-88ed-53950c4be854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bde13428b84245afecc2e22aa749b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train with no regularisation: 0.8328651685393258\n",
      "Accuracy on test with no regularisation: 0.8100558659217877\n",
      "Accuracy on train with regularisation: 0.8314606741573034\n",
      "Accuracy on test with regularisation: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "def regularisation_example():\n",
    "\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    # Define a simple neural network architecture\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    # Fit with lambda = 0 (no regularization)\n",
    "    nn_no_reg = NeuralNetwork(layers, regularisation_lambda=0)\n",
    "    nn_no_reg.initialise_weights()\n",
    "    nn_no_reg.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_no_reg = nn_no_reg.predict(X_train)\n",
    "    test_Y_pred_no_reg = nn_no_reg.predict(X_test)\n",
    "\n",
    "    # Fit with lambda = 0.2 (with regularization)\n",
    "    nn_reg = NeuralNetwork(layers, regularisation_lambda=1)\n",
    "    nn_reg.initialise_weights()\n",
    "    nn_reg.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_reg = nn_reg.predict(X_train)\n",
    "    test_Y_pred_reg = nn_reg.predict(X_test)\n",
    "\n",
    "    train_accuracy_no_reg = (train_Y_pred_no_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_no_reg = (test_Y_pred_no_reg == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_reg = (train_Y_pred_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_reg = (test_Y_pred_reg == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train with no regularisation: {train_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on test with no regularisation: {test_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on train with regularisation: {train_accuracy_reg}\")\n",
    "    print(f\"Accuracy on test with regularisation: {test_accuracy_reg}\")\n",
    "\n",
    "\n",
    "regularisation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc7504-902b-4ae7-8f42-8a25dd11c6eb",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "930dc3fd-f069-4f6c-be65-2a21dd0eb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0:X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(int) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                # dj/dz = dj/da * da/dz\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * self.Ds[l] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1.0 / m) * dZs[l] @ self.As[l-1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                dWs[l] += ((self.regularisation_lambda / m) * self.Ws[l])\n",
    "            dbs[l] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False, fig=None):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name=self.model_id)\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "            cost = self.cost(A, Y).item()\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                if np.isnan(cost):\n",
    "                    cost = 10\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs\n",
    "                        fig.data[-1].y = costs\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, self.Ws, self.regularisation_lambda)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dd565f1-7892-493c-aa40-8047a3d10b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8db12500a94852ab42b687273bb0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train with no regularisation: 0.8370786516853933\n",
      "Accuracy on test with no regularisation: 0.8044692737430168\n",
      "Accuracy on train with regularisation: 0.827247191011236\n",
      "Accuracy on test with regularisation: 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "def dropout_example():\n",
    "\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    # Define a simple neural network architecture\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    # Fit with lambda = 0 (no regularization)\n",
    "    nn_no_reg = NeuralNetwork(layers, keep_prob=1, model_id=\"No dropout\")\n",
    "    nn_no_reg.initialise_weights()\n",
    "    nn_no_reg.train(X_train, y_train, n_epochs=500, log_every=500, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_no_reg = nn_no_reg.predict(X_train)\n",
    "    test_Y_pred_no_reg = nn_no_reg.predict(X_test)\n",
    "\n",
    "    # Fit with lambda = 0.2 (with regularization)\n",
    "    nn_reg = NeuralNetwork(layers, keep_prob=0.8, model_id=\"20% drop out\")\n",
    "    nn_reg.initialise_weights()\n",
    "    nn_reg.train(X_train, y_train, n_epochs=500, log_every=50, plot_cost=True, fig=fig)\n",
    "    train_Y_pred_reg = nn_reg.predict(X_train)\n",
    "    test_Y_pred_reg = nn_reg.predict(X_test)\n",
    "\n",
    "    train_accuracy_no_reg = (train_Y_pred_no_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_no_reg = (test_Y_pred_no_reg == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_reg = (train_Y_pred_reg == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_reg = (test_Y_pred_reg == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train with no regularisation: {train_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on test with no regularisation: {test_accuracy_no_reg}\")\n",
    "    print(f\"Accuracy on train with regularisation: {train_accuracy_reg}\")\n",
    "    print(f\"Accuracy on test with regularisation: {test_accuracy_reg}\")\n",
    "\n",
    "\n",
    "dropout_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055148f-4770-4074-8549-680b3a1efba0",
   "metadata": {},
   "source": [
    "### Standard scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d4423e2-0ade-4c00-9a53-bda96bbde2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35082f92ba9b451cbba56d5f5c88b561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train: 0.8356741573033708\n",
      "Accuracy on test: 0.8100558659217877\n",
      "Accuracy on train with scaled data: 0.8455056179775281\n",
      "Accuracy on test with scaled data: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class StandardScaler:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.means: Optional[NDArray] = None\n",
    "        self.stds: Optional[NDArray] = None\n",
    "        self.is_fitted: bool = False\n",
    "\n",
    "    def fit_transform(self, X) -> NDArray:\n",
    "        self.means = X.mean(axis=1, keepdims=True)\n",
    "        self.stds = X.std(axis=1, keepdims=True)\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "    def transform(self, X) -> NDArray:\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "    def fit(self, X: NDArray) -> None:\n",
    "        self.means = X.mean(axis=1, keepdims=True)\n",
    "        self.stds = X.std(axis=1, keepdims=True)\n",
    "        self.is_fitted = True\n",
    "\n",
    "def standard_scaling_example():\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit_transform(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    layers = [30, 50, 20, 1]\n",
    "\n",
    "    nn = NeuralNetwork(layers, model_id=\"unscaled data\")\n",
    "    nn_scaled = NeuralNetwork(layers, model_id=\"scaled data\")\n",
    "    nn.initialise_weights()\n",
    "    nn_scaled.initialise_weights()\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "    nn.train(X_train, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "    nn_scaled.train(X_train_scaled, y_train, n_epochs=400, log_every=500, plot_cost=True, fig=fig)\n",
    "\n",
    "    train_Y_pred = nn.predict(X_train)\n",
    "    test_Y_pred = nn.predict(X_test)\n",
    "    train_Y_pred_scaled = nn_scaled.predict(X_train_scaled)\n",
    "    test_Y_pred_scaled = nn_scaled.predict(X_test_scaled)\n",
    "\n",
    "    train_accuracy = (train_Y_pred == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy = (test_Y_pred == y_test).sum() / y_test.shape[1]\n",
    "    train_accuracy_scaled = (train_Y_pred_scaled == y_train).sum() / y_train.shape[1]\n",
    "    test_accuracy_scaled = (test_Y_pred_scaled == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"Accuracy on train: {train_accuracy}\")\n",
    "    print(f\"Accuracy on test: {test_accuracy}\")\n",
    "    print(f\"Accuracy on train with scaled data: {train_accuracy_scaled}\")\n",
    "    print(f\"Accuracy on test with scaled data: {test_accuracy_scaled}\")\n",
    "\n",
    "standard_scaling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087ef4da-66b2-4b89-b0f7-719e2ede8fe8",
   "metadata": {},
   "source": [
    "### Mini-batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b79d59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfde2e568dbb4ae295a5723fa273779a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [], 'layout': {'template': '...'}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: Gradient Descent\n",
      "Gradient Descent training time: 9.0928 seconds\n",
      "Accuracy on train: 0.8328651685393258\n",
      "Accuracy on test: 0.8100558659217877\n",
      "\n",
      " Optimiser: Mini-batch\n",
      "Mini-batch training time: 13.5394 seconds\n",
      "Accuracy on train: 0.8384831460674157\n",
      "Accuracy on test: 0.8100558659217877\n",
      "\n",
      " Optimiser: Momentum\n",
      "Momentum training time: 10.1611 seconds\n",
      "Accuracy on train: 0.8370786516853933\n",
      "Accuracy on test: 0.8100558659217877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_18614/328717897.py:28: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log\n",
      "\n",
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_18614/328717897.py:28: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in matmul\n",
      "\n",
      "/Users/simonward-jones/Library/Caches/pypoetry/virtualenvs/statisitics-PieJMifL-py3.10/lib/python3.10/site-packages/jupyter_client/session.py:721: UserWarning:\n",
      "\n",
      "Message serialization failed with:\n",
      "Out of range float values are not JSON compliant\n",
      "Supporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimiser: RMSProp\n",
      "RMSProp training time: 15.1471 seconds\n",
      "Accuracy on train: 0.9213483146067416\n",
      "Accuracy on test: 0.8212290502793296\n",
      "\n",
      " Optimiser: ADAM\n",
      "ADAM training time: 13.0351 seconds\n",
      "Accuracy on train: 0.9283707865168539\n",
      "Accuracy on test: 0.8044692737430168\n",
      "\n",
      " Optimiser: ADAM with learning rate decay\n",
      "ADAM with learning rate decay training time: 12.5911 seconds\n",
      "Accuracy on train: 0.8469101123595506\n",
      "Accuracy on test: 0.8156424581005587\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    layer_activations: dict[int, str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        regularisation_lambda: float = 0.0,\n",
    "        keep_prob: float = 1.0,\n",
    "        cost_function: str = \"log_loss\",\n",
    "        model_id: str = \"\",\n",
    "    ):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.regularisation_lambda = regularisation_lambda\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.keep_prob = keep_prob\n",
    "        self.model_id = model_id\n",
    "        if layer_activations:\n",
    "            self.layer_activations = layer_activations\n",
    "        else:\n",
    "            # This sets all hidden layers and the output layer to \"sigmoid\" by default.\n",
    "            self.layer_activations = {l: \"sigmoid\" for l in range(1, self.L)} | {\n",
    "                self.L: \"sigmoid\"\n",
    "            }\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.params = {}\n",
    "        for l, (n_l, n_l_minus_1) in enumerate(\n",
    "            zip(self.layer_sizes[1:], self.layer_sizes), start=1\n",
    "        ):\n",
    "            self.params[f\"W{l}\"] = np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(\n",
    "                2 / n_l_minus_1\n",
    "            )\n",
    "            self.params[f\"b{l}\"] = np.zeros((n_l, 1))\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.params=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As, Ds = {}, {0: X}, {}\n",
    "        for l in range(1, self.L + 1):\n",
    "            W = self.params[f\"W{l}\"]\n",
    "            b = self.params[f\"b{l}\"]\n",
    "            Zs[l] = W @ As[l - 1] + b\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "            # apply drop out but not on the output layer\n",
    "            if cache and l != self.L:\n",
    "                Ds[l] = (np.random.uniform(size=As[l].shape) < self.keep_prob).astype(\n",
    "                    int\n",
    "                ) / self.keep_prob\n",
    "                As[l] *= Ds[l]\n",
    "        if cache:\n",
    "            self.Zs, self.As, self.Ds = Zs, As, Ds\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y):\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        grads = {}\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                W_l_plus_1 = self.params[f\"W{l+1}\"]\n",
    "                dZs[l] = (\n",
    "                    W_l_plus_1.T\n",
    "                    @ dZs[l + 1]\n",
    "                    * self.Ds[l]\n",
    "                    * ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](\n",
    "                        self.Zs[l]\n",
    "                    )\n",
    "                )\n",
    "            grads[f\"dW{l}\"] = (1.0 / m) * dZs[l] @ self.As[l - 1].T\n",
    "            if self.regularisation_lambda and l != self.L:\n",
    "                grads[f\"dW{l}\"] += (self.regularisation_lambda / m) * self.params[\n",
    "                    f\"W{l}\"\n",
    "                ]\n",
    "            grads[f\"db{l}\"] = (1.0 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def cost(self, A, Y) -> float:\n",
    "        # For cost, we need to pass the weights. We'll extract them from self.params.\n",
    "        Ws = {l: self.params[f\"W{l}\"] for l in range(1, self.L + 1)}\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y, Ws, self.regularisation_lambda).item()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Incorrect value for self.cost_function:= {self.cost_function}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat > 0.5, 1, 0)\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    \"\"\"This implements gradient descent. With optional mini-batching.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate: float = 0.1, batch_size: int | None = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        \"\"\"Update model parameters using grads returned from backward.\"\"\"\n",
    "        for param_key in model.params:\n",
    "            model.params[param_key] -= self.learning_rate * grads[f\"d{param_key}\"]\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        model,\n",
    "        X,\n",
    "        Y,\n",
    "        n_epochs=1000,\n",
    "        log_every: int | None = None,\n",
    "        plot_cost=False,\n",
    "        fig=None,\n",
    "        plot_every=10,\n",
    "    ):\n",
    "        costs, epochs = [], []\n",
    "\n",
    "        if plot_cost:\n",
    "            if fig is None:\n",
    "                fig = go.FigureWidget()\n",
    "                display(fig)\n",
    "            fig.add_scatter(x=[], y=[], mode=\"lines+markers\", name=model.model_id)\n",
    "            fig.update_layout(\n",
    "                title=\"Training Cost over Epochs\",\n",
    "                xaxis_title=\"Epoch\",\n",
    "                yaxis_title=\"Cost\",\n",
    "            )\n",
    "\n",
    "        m = X.shape[1]\n",
    "        batch_size = self.batch_size if self.batch_size is not None else m\n",
    "\n",
    "        training_iteration = 1\n",
    "        for epoch in range(n_epochs):\n",
    "            for i in range(0, m, batch_size):\n",
    "                X_batch = X[:, i : i + batch_size]\n",
    "                Y_batch = Y[:, i : i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                A = model.forward(X_batch, cache=True)\n",
    "                grads = model.backward(Y_batch)\n",
    "                self.update_model_params(model, grads, training_iteration)\n",
    "                training_iteration += 1\n",
    "\n",
    "            # Compute cost on the whole dataset after epoch\n",
    "            A_full = model.forward(X, cache=False)\n",
    "            cost = model.cost(A_full, Y)\n",
    "            if log_every and epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost and fig is not None:\n",
    "                costs.append(cost)\n",
    "                epochs.append(epoch + 1)\n",
    "                if epoch % plot_every == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[-1].x = epochs  # type: ignore\n",
    "                        fig.data[-1].y = costs  # type: ignore\n",
    "\n",
    "\n",
    "class MomentumOptimizer(Optimizer):\n",
    "    \"\"\"Implements momentum optimizer.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta * v_{t-1} + grad_t\n",
    "    param_t = param_{t-1} - learning_rate * v_t\n",
    "\n",
    "    Where:\n",
    "    - S_t is the second moment of the gradient\n",
    "    - param_t is the parameter\n",
    "    - grad_t is the gradient of the cost function with respect to the parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.cache:\n",
    "                self.cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.cache[param_key] = (\n",
    "                self.beta * self.cache[param_key]\n",
    "                + (1 - self.beta) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            model.params[param_key] -= self.learning_rate * self.cache[param_key]\n",
    "\n",
    "\n",
    "class RMSPropOptimizer(Optimizer):\n",
    "    \"\"\"Implements RMSProp optimizer.\n",
    "\n",
    "    Here we track the exponentially weighted average of the squared gradients (second moment).\n",
    "\n",
    "    The update rule is:\n",
    "    S_t = beta * S_{t-1} + (1 - beta) * (grad_t)^2\n",
    "    param_t = param_{t-1} - learning_rate * grad_t / (sqrt(S_t) + epsilon)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta: float = 0.9,\n",
    "        epsilon: float = 1e-8,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        # Standard RMSProp does NOT use bias correction (unlike Adam)\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.s_cache:\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.s_cache[param_key] = (\n",
    "                self.beta * self.s_cache[param_key]\n",
    "                + (1 - self.beta) * (grads[f\"d{param_key}\"] ** 2)\n",
    "            )\n",
    "            model.params[param_key] -= (\n",
    "                self.learning_rate\n",
    "                * grads[f\"d{param_key}\"]\n",
    "                / (np.sqrt(self.s_cache[param_key]) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "class ADAMOptimizer(Optimizer):\n",
    "    \"\"\"Implements ADAM optimizer with learning rate decay.\n",
    "\n",
    "    The update rule is:\n",
    "    v_t = beta_1 * v_{t-1} + (1 - beta_1) * grad\n",
    "    s_t = beta_2 * s_{t-1} + (1 - beta_2) * (grad_t)^2\n",
    "    v_t_corrected = v_t / (1 - beta_1^t)\n",
    "    s_t_corrected = s_t / (1 - beta_2^t)\n",
    "    param_t = param_{t-1} - learning_rate * v_t_corrected / (sqrt(s_t_corrected) + epsilon)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.1,\n",
    "        batch_size: int | None = None,\n",
    "        beta_1: float = 0.9,\n",
    "        beta_2: float = 0.999,\n",
    "        epsilon: float = 1e-8,\n",
    "        learning_rate_decay: float = 0,\n",
    "    ):\n",
    "        super().__init__(learning_rate, batch_size)\n",
    "        self.beta_1 = beta_1  # for the momentum\n",
    "        self.beta_2 = beta_2  # for the second moment (RMSProp)\n",
    "        self.epsilon = epsilon  # to avoid division by zero\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.v_cache = {}\n",
    "        self.s_cache = {}\n",
    "\n",
    "    def update_model_params(self, model, grads, training_iteration):\n",
    "        learning_rate = self.learning_rate * (\n",
    "            1 / (1 + self.learning_rate_decay * training_iteration)\n",
    "        )\n",
    "        for param_key in model.params:\n",
    "            if param_key not in self.v_cache:\n",
    "                self.v_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "                self.s_cache[param_key] = np.zeros_like(model.params[param_key])\n",
    "            self.v_cache[param_key] = (\n",
    "                self.beta_1 * self.v_cache[param_key]\n",
    "                + (1 - self.beta_1) * grads[f\"d{param_key}\"]\n",
    "            )\n",
    "            self.s_cache[param_key] = (\n",
    "                self.beta_2 * self.s_cache[param_key]\n",
    "                + (1 - self.beta_2) * grads[f\"d{param_key}\"] ** 2\n",
    "            )\n",
    "            v_t_corrected = self.v_cache[param_key] / (1 - self.beta_1 ** training_iteration)\n",
    "            s_t_corrected = self.s_cache[param_key] / (1 - self.beta_2 ** training_iteration)\n",
    "\n",
    "            model.params[param_key] -= (\n",
    "                learning_rate * v_t_corrected / (np.sqrt(s_t_corrected) + self.epsilon)\n",
    "            )\n",
    "\n",
    "\n",
    "def optimizer_example():\n",
    "    X_train = pd.read_feather(\"../titanic/processed/X_train.feather\").to_numpy().T\n",
    "    y_train = pd.read_feather(\"../titanic/processed/y_train.feather\").to_numpy().T\n",
    "    X_test = pd.read_feather(\"../titanic/processed/X_test.feather\").to_numpy().T\n",
    "    y_test = pd.read_feather(\"../titanic/processed/y_test.feather\").to_numpy().T\n",
    "\n",
    "    layers = [30, 50, 20, 1]\n",
    "    nn = NeuralNetwork(layers, model_id=\"Using an optimizer\")\n",
    "    nn_mini_batch = NeuralNetwork(layers, model_id=\"Using a mini-batch optimizer\")\n",
    "    nn_momentum = NeuralNetwork(layers, model_id=\"Using a momentum optimizer\")\n",
    "    nn_rmsprop = NeuralNetwork(layers, model_id=\"Using a RMSProp optimizer\")\n",
    "    nn_adam = NeuralNetwork(layers, model_id=\"Using a ADAM optimizer\")\n",
    "    nn_adam_decay = NeuralNetwork(layers, model_id=\"Using a ADAM optimizer with learning rate decay\")\n",
    "\n",
    "    nn.initialise_weights()\n",
    "    nn_mini_batch.initialise_weights()\n",
    "    nn_momentum.initialise_weights()\n",
    "    nn_rmsprop.initialise_weights()\n",
    "    nn_adam.initialise_weights()\n",
    "    nn_adam_decay.initialise_weights()\n",
    "\n",
    "    optimizer = Optimizer(learning_rate=0.5)\n",
    "    mini_batch_optimizer = Optimizer(batch_size=128)\n",
    "    momentum_optimizer = MomentumOptimizer(batch_size=128)\n",
    "    rmsprop_optimizer = RMSPropOptimizer(batch_size=128)\n",
    "    adam_optimizer = ADAMOptimizer(batch_size=128 )\n",
    "    adam_decay_optimizer = ADAMOptimizer(batch_size=128, learning_rate_decay=0.5)\n",
    "\n",
    "    models = [nn, nn_mini_batch, nn_momentum, nn_rmsprop, nn_adam, nn_adam_decay]\n",
    "    optimizers = [optimizer, mini_batch_optimizer, momentum_optimizer, rmsprop_optimizer, adam_optimizer, adam_decay_optimizer]\n",
    "    names = [\"Gradient Descent\", \"Mini-batch\", \"Momentum\", \"RMSProp\", \"ADAM\", \"ADAM with learning rate decay\"]\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    display(fig)\n",
    "\n",
    "    from time import perf_counter\n",
    "\n",
    "    for model, optimizer, name in zip(models, optimizers, names):\n",
    "        start_time = perf_counter()\n",
    "        optimizer.train(model, X_train, y_train, n_epochs=500, plot_cost=True, fig=fig)\n",
    "        end_time = perf_counter()\n",
    "        time = end_time - start_time\n",
    "        print(f\"\\n Optimiser: {name}\")\n",
    "        print(f\"{name} training time: {time:.4f} seconds\")\n",
    "        train_Y_pred = model.predict(X_train)\n",
    "        test_Y_pred = model.predict(X_test)\n",
    "        train_accuracy = (train_Y_pred == y_train).sum() / y_train.shape[1]\n",
    "        test_accuracy = (test_Y_pred == y_test).sum() / y_test.shape[1]\n",
    "        print(f\"Accuracy on train: {train_accuracy}\")\n",
    "        print(f\"Accuracy on test: {test_accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "optimizer_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cd2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
