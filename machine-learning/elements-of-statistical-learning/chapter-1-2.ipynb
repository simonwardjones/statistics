{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book is about learning from data. \n",
    "\n",
    "In a typical **supervised** scenario, we have outcomes (dependent/output/response variable), usually quantitative/categorical that we wish to predict based on a set of features (independent variables/inputs/predictors). We have a training set of data, in which we observe the outcome and feature. We build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. \n",
    "\n",
    "In the **unsupervised** learning problem, we observe only the features and have no measurements of the outcome. Our task is to describe how the data are organized or clustered.\n",
    "\n",
    "Examples:\n",
    " - Email spam classification\n",
    " - Prostate cancer regression prediction of log prostate specific antigen\n",
    " - Handwritten digit recognition\n",
    " - DNA unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning we use the term `regression` when the output is quantitative and `classification` when the output is qualitative (a.k.a categorical/discrete). A third variable type is ordered categorical, such as small, medium and large.\n",
    "\n",
    "Given X, make a good prediction of the output Y, denoted by $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the relationship between X and Y as a linear function of X. We include a 1 in the X vector to account for the intercept. (here X is a single vector, but in general it can be a matrix where each row is a sample and each column is a feature, in this case Y would be a vector of outcomes)\n",
    "$$\\hat{Y}  = \\hat{\\beta}_0 + \\sum_{j=1}^pX_j\\hat{\\beta}_j\\\\ = X^{T}\\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model the most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n",
    "Minimising this gives the least squares estimates of the coefficients \n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest-Neighbor Methods\n",
    "We use the k-nearest neighbors to predict the outcome of a new sample. \n",
    "$$\\hat{Y}(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)}y_i$$\n",
    "where $N_k(x)$ is the set of k points in the training set closest to x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Theory\n",
    "\n",
    "In general we seek function $f(X)$ predicting Y. We define the loss function $L(Y, f(X))$ which measures the cost of predicting $f(X)$ when the true value is Y. The most common loss function is the squared error loss \n",
    "$$L(Y, f(X)) = (Y - f(X))^2$$\n",
    "The value of $f(X)$ that minimizes the expected prediction error is the conditional expectation of Y given X, $f(x) = E(Y|X=x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of dimensionality\n",
    "\n",
    "Looking at the nearest neighbours method, as the number of dimensions increases, the volume of the space increases so that the nearest neighbours are no longer as close. This is the curse of dimensionality and increases the bias of the nearest neighbour method.\n",
    "\n",
    "The book shows that by leveraging structure of the problem, such as linearity, we can reduce the impact of the curse of dimensionality reducing the bias and variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Supervised Learning and Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Often we can reframe the supervised problem as a statistical model where\n",
    "$$\n",
    "Y = f(X) + \\epsilon\n",
    "$$\n",
    "where $E(\\epsilon) = 0$\n",
    "\n",
    "We want to estimate $f(X)$ using the training data. We can view this as a `learning problem` where we iteratively improve our estimate of $f(X)$ or as a `function approximation problem`.\n",
    "\n",
    "To find the optimal function we often minimize the square loss but a more general approach is to maximised the likelihood of the data given some assumed model. In the case of a linear model assuming the errors are normally distributed we can show maximising the likelihood is equivalent to minimizing the square loss.\n",
    "\n",
    "Besides a linear model or nearest neighbours other methods include:\n",
    " - `Roughness penalty methods`: These reduce model complexity by penalizing the complexity of the model in the loss function. This is also known as `regularization`.\n",
    " - `Kernel methods`, similar to nearest neighbours but with a `weighted average of the neighbours` to account for the distance.\n",
    " - `Basis functions` assume f is of the form $\\sum_{j=1}^M\\theta_jh_j(x)$ where $h_j(x)$ are functions of x. These approaches linear models, splines, single layer neural networks and radial basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bias variance tradeoff\n",
    "\n",
    "Many models have a smoothing parameter that controls the complexity of the model. The choice of this parameter is a tradeoff between bias and variance. A model with high bias will underfit the data, while a model with high variance will overfit the data.\n",
    "\n",
    "The expected prediction error can be decomposed into the irreducible error, the squared bias and the variance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
