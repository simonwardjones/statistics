{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book is about learning from data. \n",
    "\n",
    "In a typical **supervised** scenario, we have outcomes (dependent/output/response variable), usually quantitative/categorical that we wish to predict based on a set of features (independent variables/inputs/predictors). We have a training set of data, in which we observe the outcome and feature. We build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. \n",
    "\n",
    "In the **unsupervised** learning problem, we observe only the features and have no measurements of the outcome. Our task is to describe how the data are organized or clustered.\n",
    "\n",
    "Examples:\n",
    " - Email spam classification\n",
    " - Prostate cancer regression prediction of log prostate specific antigen\n",
    " - Handwritten digit recognition\n",
    " - DNA unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning we use the term `regression` when the output is quantitative and `classification` when the output is qualitative (a.k.a categorical/discrete). A third variable type is ordered categorical, such as small, medium and large.\n",
    "\n",
    "Given X, make a good prediction of the output Y, denoted by $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the relationship between X and Y as a linear function of X. We include a 1 in the X vector to account for the intercept. (here X is a single vector, but in general it can be a matrix where each row is a sample and each column is a feature, in this case Y would be a vector of outcomes)\n",
    "$$\\hat{Y}  = \\hat{\\beta}_0 + \\sum_{j=1}^pX_j\\hat{\\beta}_j\\\\ = X^{T}\\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model the most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n",
    "Minimising this gives the least squares estimates of the coefficients \n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest-Neighbor Methods\n",
    "We use the k-nearest neighbors to predict the outcome of a new sample. \n",
    "$$\\hat{Y}(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)}y_i$$\n",
    "where $N_k(x)$ is the set of k points in the training set closest to x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Theory\n",
    "\n",
    "In general we seek function $f(X)$ predicting Y. We define the loss function $L(Y, f(X))$ which measures the cost of predicting $f(X)$ when the true value is Y. The most common loss function is the squared error loss \n",
    "$$L(Y, f(X)) = (Y - f(X))^2$$\n",
    "The value of $f(X)$ that minimizes the expected prediction error is the conditional expectation of Y given X, $f(x) = E(Y|X=x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of dimensionality\n",
    "\n",
    "Looking at the nearest neighbours method, as the number of dimensions increases, the volume of the space increases so that the nearest neighbours are no longer as close. This is the curse of dimensionality and increases the bias of the nearest neighbour method.\n",
    "\n",
    "The book shows that by leveraging structure of the problem, such as linearity, we can reduce the impact of the curse of dimensionality reducing the bias and variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Supervised Learning and Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Often we can reframe the supervised problem as a statistical model where\n",
    "$$\n",
    "Y = f(X) + \\epsilon\n",
    "$$\n",
    "where $E(\\epsilon) = 0$\n",
    "\n",
    "We want to estimate $f(X)$ using the training data. We can view this as a `learning problem` where we iteratively improve our estimate of $f(X)$ or as a `function approximation problem`.\n",
    "\n",
    "To find the optimal function we often minimize the square loss but a more general approach is to maximised the likelihood of the data given some assumed model. In the case of a linear model assuming the errors are normally distributed we can show maximising the likelihood is equivalent to minimizing the square loss.\n",
    "\n",
    "Besides a linear model or nearest neighbours other methods include:\n",
    " - `Roughness penalty methods`: These reduce model complexity by penalizing the complexity of the model in the loss function. This is also known as `regularization`.\n",
    " - `Kernel methods`, similar to nearest neighbours but with a `weighted average of the neighbours` to account for the distance.\n",
    " - `Basis functions` assume f is of the form $\\sum_{j=1}^M\\theta_jh_j(x)$ where $h_j(x)$ are functions of x. These approaches linear models, splines, single layer neural networks and radial basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bias variance tradeoff\n",
    "\n",
    "Many models have a smoothing parameter that controls the complexity of the model. The choice of this parameter is a tradeoff between bias and variance. A model with high bias will underfit the data, while a model with high variance will overfit the data.\n",
    "\n",
    "The expected prediction error can be decomposed into the irreducible error, the squared bias and the variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Chapter 3 - Linear methods for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assume $E(Y|X)$ is linear in $X_i$. We have an input vector $X^T = (X_1, X_2, ..., X_p)$ and we want to predict a real-valued output Y.\n",
    "$$\n",
    "    f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j\n",
    "$$\n",
    "We have a training set of N observations $(x_1, y_1), ..., (x_N, y_N)$ and we want to estimate the coefficients $\\beta_0, \\beta_1, ..., \\beta_p$. Each $x_i$ is a p-dimensional vector $x_i = (x_{i1}, x_{i2}, ..., x_{ip})^T$.\n",
    "The most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this we can differentiate the RSS with respect to $\\beta$ and set to zero\n",
    "$$\n",
    "    \\frac{\\partial RSS}{\\partial \\beta} = -2X^T(y - X\\beta) = 0\n",
    "$$\n",
    "This gives the normal equations\n",
    "$$\n",
    "    X^TX\\beta = X^Ty\n",
    "$$\n",
    "and the least squares estimate of $\\beta$ is\n",
    "$$\n",
    "    \\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values for the training data are\n",
    "$$\n",
    "    \\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "We denote the matrix $H = X(X^TX)^{-1}X^T$ as the `hat matrix` because it puts a `hat` on y. Geometrically this matrix projects y orthogonally onto the space spanned by the columns of X.\n",
    "\n",
    "$$\n",
    "    var(\\hat{\\beta}) = var((X^TX)^{-1}X^Ty) = (X^TX)^{-1}X^Tvar(y)((X^TX)^{-1}X^T)^T = \\\\\n",
    "     = (X^TX)^{-1}X^Tvar(y)X(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}\n",
    "$$\n",
    "\n",
    "Typically we don't know $\\sigma^2$ so we estimate it with the residual sum of squares\n",
    "$$\n",
    "    \\hat{\\sigma}^2 = \\frac{1}{N-p-1}\\sum_{i=1}^N(y_i - x_i^T\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "Assuming the errors are normal and the model is correct then\n",
    "$$\n",
    "\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{(N-p-1)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{N-p-1}\n",
    "$$\n",
    "We can use this to construct confidence intervals for the coefficients and to test hypotheses about the coefficients. We can calculate the $z_j$ values which follow a t-distribution with N-p-1 degrees of freedom and allow us to test the null hypothesis $\\beta_j = 0$.\n",
    "$$\n",
    "z_j = \\frac{\\hat{\\beta}_j}{\\sqrt{\\hat{\\sigma}^2((X^TX)^{-1})_{jj}}}\n",
    "$$\n",
    "Note the standard error of $\\hat{\\beta}_j$ is $\\sqrt{\\hat{\\sigma}^2((X^TX)^{-1})_{jj}}$\n",
    "\n",
    "#### gauss markov theorem\n",
    "The least squares estimates are the best linear unbiased estimates (BLUE) of the coefficients. This means they have the smallest variance of all linear unbiased estimates. A linear unbiased estimate is one that is unbiased and is a linear function of the data. By linear we mean of the form $c^Ty$ where c depends on the data but not on the parameter being estimated. Unbiased means $E(c^Ty) = c^TE(y) = c^TX\\beta$.\n",
    "\n",
    "The Gram-Schmidt procedure can be used to orthogonalize the columns of X. This can be useful when the columns are highly correlated. The multiple regression coefficient $\\beta_j$ represents the marginal effect of $X_j$ on Y after adjusting for all other variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Subset selection\n",
    "\n",
    " - *Best Subset selection* fits regressions for every subset of features finding the one with the minimum least squares error.\n",
    " - *Forward and stepwise selection* starts with just an intercept and adds features one at a time. Forward selection adds the feature that most reduces the RSS. Backward selection starts with all features and removes the one that least increases the RSS.\n",
    " - *Forward stagewise* regression starts like forward stepwise regression fitting just the intercept. At it step it finds the variable most correlated with the current residual. It then adds the simple linear regression coefficient of this variable to the current coefficient. This is repeated until no variable has a correlation with the residual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression\n",
    "\n",
    "Beta is chosen to minimize the penalized residual sum of squares\n",
    "\n",
    "$$\n",
    "    \\hat{\\beta}_{ridge} = \\underset{\\beta}{\\mathrm{argmin}} \\left\\{ \\sum_{i=1}^N(y_i - x_i^T\\beta)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 \\right\\}\n",
    "$$\n",
    "\n",
    "In vector notation\n",
    "$$\n",
    "    RSS(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda\\beta^T\\beta\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\hat{\\beta}_{ridge} = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at SVD \n",
    "\n",
    "The singular value decomposition of X is \n",
    "$$\n",
    "    X = UDV^T\n",
    "$$\n",
    "Where U is an N * P orthogonal matrix, D is a P * P diagonal matrix where d_1 >= d_2 >= ... >= d_p >= 0 and V is a P * P orthogonal matrix.\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X\\hat{\\beta}_{LS} &= X(X^TX)^{-1}X^Ty \\\\\n",
    "    &= UDV^T((UDV^T)^TUDV^T)^{-1}(UDV^T)^Ty \\\\\n",
    "    &= UDV^T(VDU^TUDV^T)^{-1}VDU^Ty \\\\\n",
    "    &= UDV^T(VD^2V^T)^{-1}VDU^Ty \\\\\n",
    "    &= UDV^TV(D^2)^{-1}V^TVDU^Ty \\\\\n",
    "    &= UD(D^2)^{-1}DU^Ty \\\\\n",
    "    &= UU^Ty\n",
    "\\end{align*}\n",
    "$$\n",
    "And the ridge regression solution is\n",
    "$$\n",
    "    X\\hat{\\beta}_{ridge} = UD(D^2 + \\lambda I)^{-1}DU^Ty \\\\\n",
    "    = \\sum_{j=1}^pu_j\\frac{d_j^2}{d_j^2 + \\lambda}u_j^Ty\n",
    "$$\n",
    "Where $u_j$ is the jth column of U and $d_j$ is the jth diagonal element of D.\n",
    "Note this shows the solution projects y onto the space spanned by the columns of U (so X) and shrinks the coefficients by a factor of $\\frac{d_j^2}{d_j^2 + \\lambda}$. Note the $u_j$ are the principal components of X and due to the factor the ridge regression shrinks the coefficients of the principal components with the smallest variance the most.\n",
    "\n",
    "**Relation to sample covariance**\n",
    "\n",
    "The sample covariance matrix of X is $S = X^TX/N$. Note \n",
    "$$\n",
    "    X^TX = (UDV^T)^TUDV^T = VDU^TUDV^T = VD^2V^T\n",
    "$$\n",
    "which is the eigen decomposition of $X^TX$. The eigenvectors of $X^TX$ are the columns of V and the eigenvalues are the squares of the singular values of X. $z_i = Xv_i$ are the principal components of X.\n",
    "$$\n",
    "   z_i = Xv_i = UDV^Tv_i = UD\\mathbb{1}_{j=i} = d_iu_i\n",
    "$$\n",
    "The first principal component is the direction in which the data varies the most. The second principal component is the direction orthogonal to the first in which the data varies the most and so on. The variance of the data in the direction of the ith principal component is $d_i^2/N$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso regression\n",
    "\n",
    "It is similar to ridge regression but uses the L1 norm instead of the L2 norm. This leads to sparsity in the coefficients. The lasso estimate is\n",
    "$$\n",
    "    \\hat{\\beta}_{lasso} = \\underset{\\beta}{\\mathrm{argmin}} \\left\\{ \\sum_{i=1}^N(y_i - x_i^T\\beta)^2 + \\lambda\\sum_{j=1}^p|\\beta_j| \\right\\}\n",
    "$$\n",
    "or in verctor notation\n",
    "$$\n",
    "    RSS(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda||\\beta||_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic net\n",
    "\n",
    "The elastic net is a combination of the lasso and ridge regression. It has two tuning parameters $\\alpha$ and $\\lambda$ and\n",
    "$$\n",
    "    RSS(\\beta) = (y - X\\beta)^T(y - X\\beta) + \\lambda\\left(\\alpha||\\beta||_1 + (1-\\alpha)||\\beta||_2^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least angle regression\n",
    "\n",
    "Least angle regression is a method for fitting a linear model with a large number of features. It is somewhat similar to forward stepwise regression. First the matrix X is normalised so that each predictor has mean 0 and variance 1.\n",
    "\n",
    "To start it sets all Beta coefficients to 0 and finds the variable $x_i$ most correlated with y, this makes up the current active set. For all variables in the active set it moves the coefficients towards the least squares solution for the current active set and the residual. When another variable becomes as correlated with the residual as one of the active variables it adds it to the active set.\n",
    "\n",
    "Lasso modification: If a non-zero coefficient hits zero then it is removed from the active set and the update direction is recalculated.\n",
    "\n",
    "#### Degrees of freedom\n",
    "\n",
    "A more general definition of the degrees of freedom can be made as\n",
    "$$\n",
    "    df(\\hat{y}) = \\frac{1}{\\sigma^2}\\sum_{i=1}^Ncov(\\hat{y}_i, y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Principle Components regression\n",
    "\n",
    "Principle component regression regresses Y on the first M principle components of X, $z_1, z_2, ..., z_m$ . This regression leads to projections\n",
    "$$\n",
    "    \\hat{y}^{PCR}_{(M)} = \\bar{y}\\mathbb{1} + \\sum_{m=1}^M\\hat{\\theta}_mz_m\n",
    "$$\n",
    "The coefficients $\\hat{\\theta}_m$ are found by regressing y on the principle components and as they are orthogonal $\\theta_m = \\frac{<z_m, y>}{<z_m, z_m>} = \\frac{z_m^Ty}{z_m^Tz_m}$\n",
    "\n",
    "We see that principal components regression is very similar to ridge regression: both operate via the principal components of the input matrix. Ridge regression shrinks the coefficients of the principal components, shrinking more depending on the size of the corresponding eigenvalue; principal components regression discards the p − M smallest eigenvalue components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Partial least squares\n",
    "\n",
    "First we assume X is normalised so that each predictor has mean 0 and variance 1. For m in 1, ...., p we create the mth partial least squares direction $z_m = \\sum_{j=1}^p\\phi_{mj}x_j^{(m-1)}$ where $\\phi_{mj} = <x_j, y>$ and $x_j^{(0)} = x_j$. We then orthogonalize each $x_j^{(m)} w.r.t $z_m$ as $x_j^{(m)} = x_j^{(m-1)} - \\frac{<x_j^{(m-1)}, z_m>}{<z_m, z_m>}z_m$. We then regress y on $z_1, z_2, ..., z_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, PLS, PCR and ridge regression tend to behave similarly.\n",
    "Ridge regression may be preferred because it shrinks smoothly, rather than\n",
    "in discrete steps. Lasso falls somewhere between ridge regression and best\n",
    "subset regression, and enjoys some of the properties of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
