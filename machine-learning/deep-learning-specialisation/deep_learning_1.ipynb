{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5f2bd7-6b62-4a6b-8a00-469e4889a63a",
   "metadata": {},
   "source": [
    "# Deep Learning Specialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3ccfb-5af2-4f31-9555-fd5b5ac70bd8",
   "metadata": {},
   "source": [
    "## Neural networks and deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef1b21d-4c74-4e09-8ac0-15addad093cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Logistic regression as neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c781a-ef01-4fb5-be97-7330282e1ca6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Inputs are images of cats a pixels of rgb and output in 0 or 1\n",
    "- m training examples (x, y) where $x\\in\\mathbb{R}^n, y\\in\\{0,1\\}$\n",
    "- Often X represents the training data where the rows are samples however in this course the columns of X are the training examples and hence has shape (n, m).\n",
    "- Y represents the vector of y values with shape (m, 1)\n",
    "- $\\hat{y}$ is the prediction. In the cat example $\\hat{y} = P(y=1|x)$\n",
    "- In logisitic regression $\\hat{y}=\\sigma(w^Tx+b)$ where $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "- Loss function $L(\\hat{y}, y) = - (y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}))$\n",
    "- Cost function $J(w, b) = \\frac{1}{m} \\sum_{i=1}^mL(\\hat{y}^i, y^i)$, J is convex\n",
    "- Gradient descent updates w and b to minimise the cost\n",
    "- We repeatedly update $w := w-\\alpha\\frac{\\partial J}{\\partial w} = w-\\alpha dw$ where alpha is the learning rate and  $b := b-\\alpha\\frac{\\partial J}{\\partial b} = b -\\alpha db$\n",
    "- When implementing we need to vectorise the implementation\n",
    "- Logistic regression diagram\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x1(($$x_1$$)) --> z(\"$$z = w^Tx + b$$\")\n",
    "    x2(($$x_2$$)) --> z\n",
    "    x3(($$x_3$$)) --> z\n",
    "    z --> a(\"$$\\hat{y} = a= \\sigma(z) $$\")\n",
    "```\n",
    "- Logistic regression equations are often summarised as\n",
    "$$\n",
    "\\begin{align*}\n",
    "z^{(i)} &= w^Tx^{(i)}+b \\\\\n",
    "\\hat{y}^{(i)} &= a^{(i)} = sigmoid(z^{(i)}) \\\\\n",
    "L(a^{(i)}, y^{(i)}) &= -y^{(i)}\\log(a^{(i)}) - (1 - y^{(i)})\\log(1-a^{(i)}) \\\\\n",
    "J &= \\frac{1}{m}\\sum_{i=1}^mL(a^{(i)} - y^{(i)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In vector form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= \\sigma(W^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)}) \\\\\n",
    "\\frac{\\partial J}{\\partial W} &= dW = \\frac{1}{m}X(A-Y)^T \\\\\n",
    "\\frac{\\partial J}{\\partial b} &= db = \\frac{1}{m}\\sum_{i=1}^m (a^{(i)}-y^{(i)}) \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T)\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "X - dim: (n, m) \\quad \n",
    "w - dim: (n, 1) \\quad\n",
    "b - dim: (1, 1) \\quad\n",
    "A - dim: (1, m) \\quad\n",
    "Y - dim: (1, m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7302c-360c-4b98-83fe-b0620096e00e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shallow neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a054117-9a86-42e9-822d-148ca6975c18",
   "metadata": {},
   "source": [
    "- For a network we use super script [i] to descripe the ith layer where n the dimension of x in $n^{[0]}$\n",
    "- network flow\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    x1(($$x_1$$)) --> z1(\"$$z^{[1]} = W^{[1]}x + b[1]$$\")\n",
    "    x2(($$x_2$$)) --> z1\n",
    "    x3(($$x_3$$)) --> z1\n",
    "    z1 --> a1(\"$$a^{[1]} = \\sigma(z^{[1]}) $$\")\n",
    "    a1 --> z2(\"$$z^{[2]} = W^{[2]}a^{[1]} + b[2]$$\")\n",
    "    z2 --> a2(\"$$\\hat{y} = a^{[2]} = \\sigma(z^{[2]}) $$\")\n",
    "```\n",
    "- The forward pass is calculated as\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{[1](i)} &= W^{[1]}x^{(i)}+b^{[1]}, \\quad\\text{i is the ith sample} \\\\\n",
    "z^{[1]} &= W^{[1]}X+b^{[1]} \\\\\n",
    "A^{[1]} &= \\sigma(z^{[1]}) \\\\ \n",
    "z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]} \\\\\n",
    "Y = A^{[2]} &= \\sigma(z^{[2]}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- Activation functions could be tanh, relu, leaky relu or the signmoid\n",
    "- Taking the example of a two layer network (note we don't include the input layer in the count so we have 1 hidden layer) we have params\n",
    "\n",
    "$$\\begin{align}\n",
    "x^{(i)} &- dim: (n, 1) \\\\\n",
    "X &- dim: (n, m) \\\\\n",
    "W^{[1]} &- dim: (n^{[1]}, n^{[0]}) \\\\\n",
    "b^{[1]} &- dim: (n^{[1]}, 1) \\\\\n",
    "Z^{[1]} &- dim: (n^{[1]}, m) \\\\\n",
    "A^{[1]} &- dim: (n^{[1]}, m) \\\\\n",
    "W^{[2]} &- dim: (n^{[2]}, n^{[1]}) \\\\\n",
    "b^{[2]} &- dim: (n^{[2]}, 1) \\\\\n",
    "Z^{[2]} &- dim: (n^{[2]}, m) \\\\\n",
    "A^{[2]} &- dim: (n^{[2]}, m) \\\\\n",
    "&n^{[2]} = 1 \\\\\n",
    "Y &- dim: (n^{[2]}, m) \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "- Back prop equations\n",
    "$$\\begin{align}\n",
    "dZ^{[2]} &= A^{[2]} - Y \\\\\n",
    "dW^{[2]} &= \\frac{1}{m}dZ^{[2]}A^{[1]T} \\\\\n",
    "db^{[2]} &= \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) \\\\\n",
    "dZ^{[1]} &= W^{[2]T}dZ^{[2]} * \\sigma'(Z^{[1]}) \\\\\n",
    "dW^{[1]} &= \\frac{1}{m}dZ^{[1]}X^T \\\\\n",
    "db^{[1]} &= \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True) \\\\\n",
    "\\end{align}\n",
    "$$ \n",
    "- initialise weights randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb499d-a799-473c-8705-ccb491d6c96f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300d15f-e027-4911-836a-a40966a21c39",
   "metadata": {},
   "source": [
    " - The equations for the forward pass can we summarised as below where there are $L$ layers\n",
    " $$\n",
    "\\begin{align}\n",
    "z^{[l](i)} &= W^{[l]}a^{[l-1](i))}+b^{[l]}, \\quad\\text{ith sample, lth layer} \\\\\n",
    "a^{[l](i)} &= g(z^{[l](i)}) , \\quad\\text{ith sample, lth layer, g - activation function} \\\\\n",
    "z^{[0](i)} &= x^{(i)} \\\\\n",
    "\\hat{y^{(i)}} &= a^{[L](i)} \\\\\n",
    "L(a^{(i)}, y^{(i)}) &= -y^{(i)}\\log(a^{(i)}) - (1 - y^{(i)})\\log(1-a^{(i)}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    " - As vectors\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]} &= W^{[l]}A^{[l-1]}+b^{[l]} \\\\\n",
    "A^{[l]} &= g(Z^{[l]}), \\quad\\text{lth layer, g - activation function} \\\\ \n",
    "Z^{[0]} &= X \\\\\n",
    "\\hat{Y} &= A^{[L]} \\\\\n",
    "J &= -\\frac{1}{m}(Ylog(A)^T + (1-Y)log(1-A)^T)\n",
    "\\end{align}\n",
    "$$\n",
    "- The equations for the backward pass\n",
    "$$\n",
    "\\begin{align}\n",
    "dZ^{[L]} &= A^{[L]} - Y \\\\\n",
    "dZ^{[l]} &= W^{[l+1]^T}dZ^{[l+1]} * g'(Z^{[l]}) \\quad l\\in\\{1...L-1\\} \\\\\n",
    "dW^{[l]} &= \\frac{1}{m}dZ^{[l]}A^{[l-1]^T} \\quad l\\in\\{1...L-1\\} \\\\\n",
    "db^{[l]} &= \\frac{1}{m}np.sum(dZ^{[l]}, axis=1, keepdims=True) \\quad l\\in\\{1...L-1\\} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc21ff5-a271-47e2-8059-f6eeed76a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "sigmoid = expit\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1.0 - sigmoid(z))\n",
    "\n",
    "def leaky_relu(x, leaky_constant:float = 0.01):\n",
    "    return np.where(x > 0.0, x,  x * leaky_constant)\n",
    "\n",
    "def leaky_relu_derivative(x, leaky_constant=0.01):\n",
    "    return np.where(x > 0, 1, leaky_constant)\n",
    "\n",
    "def log_loss(A, Y):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    return - (1/m) * (Y @ np.log(A).T + (1 - Y) @ np.log(1 - A).T)\n",
    "\n",
    "def square_loss(A, Y):\n",
    "    m = A.shape[1] # m is number samples\n",
    "    return - (1/m) * (A - Y) @ (A - Y).T\n",
    "\n",
    "ACTIVATION_FUNCTIONS : dict[str, Callable] = {\n",
    "    \"relu\": relu,\n",
    "    \"leaky_relu\": leaky_relu,\n",
    "    \"sigmoid\": expit,\n",
    "}\n",
    "\n",
    "ACTIVATION_FUNCTION_DERIVATIVES: dict[str, Callable] = {\n",
    "    \"relu\": relu_derivative,\n",
    "    \"leaky_relu\": leaky_relu_derivative,\n",
    "    \"sigmoid\": sigmoid_derivative,\n",
    "}\n",
    "\n",
    "LOSS_FUNCTIONS: dict[str, Callable] = {\n",
    "    \"log_loss\": log_loss,\n",
    "    \"square_loss\": square_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c195be8-b644-41b6-8cc2-e32973711ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],\n",
    "        learning_rate: float = 0.5,\n",
    "        layer_activations: dict[int, str] | None = None,\n",
    "        cost_function: str = \"log_loss\"\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1\n",
    "        self.m = layer_sizes[0]\n",
    "        self.cost_function = cost_function\n",
    "        self.learning_rate\n",
    "        self.layer_activations = layer_activations\n",
    "        if not layer_activations:\n",
    "            self.layer_activations = {l:\"sigmoid\" for l in range(1, self.L)} | {self.L:\"sigmoid\"}\n",
    "\n",
    "    def initialise_weights(self) -> None:\n",
    "        # Note layer indexes are off by one because python indexes by 0\n",
    "        # so Ws[0] is really W^{[1]}\n",
    "\n",
    "        # This is using He initialisation. Try changing to * 0.01 and see the change in cost plot.\n",
    "        self.Ws = {\n",
    "            l:np.random.normal(size=(n_l, n_l_minus_1)) * np.sqrt(2 / n_l_minus_1)\n",
    "            for (l, (n_l, n_l_minus_1)) in enumerate(zip(self.layer_sizes[1:], self.layer_sizes), start=1)\n",
    "        }\n",
    "        self.bs = {l:np.zeros((n_l, 1)) for l, n_l in enumerate(self.layer_sizes[1:], start=1)}\n",
    "        logger.info(\"Weights initialised\")\n",
    "        logger.debug(f\"{self.Ws=}\")\n",
    "\n",
    "    def forward(self, X, cache=False) -> None:\n",
    "        Zs, As = {}, {0:X}\n",
    "        for l in range(1, self.L + 1):\n",
    "            Zs[l] = self.Ws[l] @ As[l-1] + self.bs[l]\n",
    "            g = ACTIVATION_FUNCTIONS[self.layer_activations[l]]\n",
    "            logger.debug(f\"Applying {self.layer_activations[l]} in layer{l}\")\n",
    "            As[l] = g(Zs[l])\n",
    "        if cache:\n",
    "            self.Zs, self.As = Zs, As\n",
    "        return As[self.L]\n",
    "\n",
    "    def backward(self, Y) -> None:\n",
    "        dZs = {self.L: self.As[self.L] - Y}\n",
    "        m = self.As[0].shape[1]\n",
    "        dWs, dbs = {}, {}\n",
    "        # [w1, w2, w3]\n",
    "        for l in range(self.L, 0, -1):\n",
    "            logger.debug(f\"calculating dZ for layer_id {l}\")\n",
    "            if l != self.L:\n",
    "                dZs[l] = self.Ws[l+1].T @ dZs[l+1] * \\\n",
    "                    ACTIVATION_FUNCTION_DERIVATIVES[self.layer_activations[l]](self.Zs[l])\n",
    "                    # For sigmoid we could just use self.As[l] * (1 - self.As[l])\n",
    "            dWs[l] = (1 / m) * dZs[l] @ self.As[l-1].T\n",
    "            dbs[l] = (1 / m) * np.sum(dZs[l], axis=1, keepdims=True)\n",
    "        self.dWs, self.dbs = dWs, dbs\n",
    "\n",
    "    def update_weights(self):\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.Ws[l] -= self.learning_rate * self.dWs[l]\n",
    "            self.bs[l] -= self.learning_rate * self.dbs[l]\n",
    "\n",
    "    def train(self, X, Y, n_epochs=10, log_every=100, plot_cost=False):\n",
    "        import plotly.graph_objs as go\n",
    "        from IPython.display import display, clear_output\n",
    "\n",
    "        costs = []\n",
    "        epochs = []\n",
    "\n",
    "        if plot_cost:\n",
    "            fig = go.FigureWidget()\n",
    "            fig.add_scatter(x=[], y=[], mode='lines+markers', name='Cost')\n",
    "            fig.update_layout(title='Training Cost over Epochs', xaxis_title='Epoch', yaxis_title='Cost')\n",
    "            display(fig)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            A = self.forward(X, cache=True)\n",
    "            cost = self.cost(A, Y)\n",
    "            if epoch % log_every == 0:\n",
    "                logger.info(f\"Cost after epoch {epoch} = {cost}\")\n",
    "            if plot_cost:\n",
    "                costs.append(float(cost))\n",
    "                epochs.append(epoch)\n",
    "                if epoch % 10 == 0:\n",
    "                    with fig.batch_update():\n",
    "                        fig.data[0].x = epochs\n",
    "                        fig.data[0].y = costs\n",
    "            self.backward(Y)\n",
    "            self.update_weights()\n",
    "\n",
    "    def cost(self, A, Y):\n",
    "        if self.cost_function == \"log_loss\":\n",
    "            return log_loss(A, Y)\n",
    "        elif self.cost_function == \"square_loss\":\n",
    "            return square_loss(A, Y)\n",
    "        else:\n",
    "            raise Exception(f\"Incorrect value for self.cost_function:= {self.cost_function}\")\n",
    "\n",
    "    def predict(self, X, return_probability=False):\n",
    "        Y_hat = self.forward(X)\n",
    "        if return_probability:\n",
    "            return Y_hat\n",
    "        return np.where(Y_hat>0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5adc5b4c-8d56-4704-b3ca-9bbf99fa875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    X_train = pd.read_feather('../titanic/processed/X_train.feather').to_numpy().T\n",
    "    y_train = pd.read_feather('../titanic/processed/y_train.feather').to_numpy().T\n",
    "    X_test = pd.read_feather('../titanic/processed/X_test.feather').to_numpy().T\n",
    "    y_test = pd.read_feather('../titanic/processed/y_test.feather').to_numpy().T\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, force=True)\n",
    "\n",
    "    layer_sizes = [30, 50, 20, 1] # L = 3, A[3] = Yhat\n",
    "    neural_network = NeuralNetwork(layer_sizes=layer_sizes)\n",
    "\n",
    "    # uncomment to see relu converges better !\n",
    "    # layer_sizes = [30, 50, 1] # L = 3, A[3] = Yhat\n",
    "    # neural_network = NeuralNetwork(\n",
    "    #     layer_sizes=layer_sizes,\n",
    "    #     layer_activations={1:\"relu\", 2:\"relu\", 3:\"sigmoid\"},\n",
    "    # )\n",
    "\n",
    "    neural_network.initialise_weights()\n",
    "    neural_network.train(X_train, y_train, n_epochs=2000, plot_cost=True)\n",
    "\n",
    "    y_test_pred = neural_network.predict(X_test)\n",
    "    accuracy = (y_test_pred == y_test).sum() / y_test.shape[1]\n",
    "\n",
    "    print(f\"X_train.shape: {X_train.shape}\")\n",
    "    print(f\"layer_activations = {neural_network.layer_activations}\")\n",
    "    print(f\"L = {neural_network.L}\")\n",
    "    print(\"ws shapes:\",  [(i, w.shape) for i, w in neural_network.Ws.items()])\n",
    "    print(\"As shapes:\",  [(i, a.shape) for i, a in neural_network.As.items()])\n",
    "    print(\"zs shapes:\",  [(i, z.shape) for i, z in neural_network.Zs.items()])\n",
    "    print(f\"Accuracy on test: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da0f9dc-e0b2-4d71-a92b-70bdcbb7dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Weights initialised\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226cc12e05304c87b0aee79483dc4255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines+markers',\n",
       "              'name': 'Cost',\n",
       "              'type': 'scatter',\n",
       "              'uid': '5083a0c6-8206-4719-a84e-679b0abface0',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training Cost over Epochs'},\n",
       "               'xaxis': {'title': {'text': 'Epoch'}},\n",
       "               'yaxis': {'title': {'text': 'Cost'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Cost after epoch 0 = [[0.67791741]]\n",
      "/var/folders/wh/w1gqxy1n7fq780hh5k5wzcyc0000gq/T/ipykernel_94230/1060446905.py:87: DeprecationWarning:\n",
      "\n",
      "Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "\n",
      "INFO:root:Cost after epoch 100 = [[0.46155934]]\n",
      "INFO:root:Cost after epoch 200 = [[0.43274977]]\n",
      "INFO:root:Cost after epoch 300 = [[0.42003758]]\n",
      "INFO:root:Cost after epoch 400 = [[0.41334262]]\n",
      "INFO:root:Cost after epoch 500 = [[0.40872809]]\n",
      "INFO:root:Cost after epoch 600 = [[0.40503333]]\n",
      "INFO:root:Cost after epoch 700 = [[0.40186904]]\n",
      "INFO:root:Cost after epoch 800 = [[0.39904907]]\n",
      "INFO:root:Cost after epoch 900 = [[0.39646523]]\n",
      "INFO:root:Cost after epoch 1000 = [[0.39405118]]\n",
      "INFO:root:Cost after epoch 1100 = [[0.39176588]]\n",
      "INFO:root:Cost after epoch 1200 = [[0.38958391]]\n",
      "INFO:root:Cost after epoch 1300 = [[0.3874885]]\n",
      "INFO:root:Cost after epoch 1400 = [[0.38546622]]\n",
      "INFO:root:Cost after epoch 1500 = [[0.38350336]]\n",
      "INFO:root:Cost after epoch 1600 = [[0.38158428]]\n",
      "INFO:root:Cost after epoch 1700 = [[0.37969143]]\n",
      "INFO:root:Cost after epoch 1800 = [[0.37780638]]\n",
      "INFO:root:Cost after epoch 1900 = [[0.37591085]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (30, 712)\n",
      "layer_activations = {1: 'sigmoid', 2: 'sigmoid', 3: 'sigmoid'}\n",
      "L = 3\n",
      "ws shapes: [(1, (50, 30)), (2, (20, 50)), (3, (1, 20))]\n",
      "As shapes: [(0, (30, 712)), (1, (50, 712)), (2, (20, 712)), (3, (1, 712))]\n",
      "zs shapes: [(1, (50, 712)), (2, (20, 712)), (3, (1, 712))]\n",
      "Accuracy on test: 0.8212290502793296\n"
     ]
    }
   ],
   "source": [
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec680e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
