{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book is about learning from data. \n",
    "\n",
    "In a typical **supervised** scenario, we have outcomes (dependent/output/response variable), usually quantitative/categorical that we wish to predict based on a set of features (independent variables/inputs/predictors). We have a training set of data, in which we observe the outcome and feature. We build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. \n",
    "\n",
    "In the **unsupervised** learning problem, we observe only the features and have no measurements of the outcome. Our task is to describe how the data are organized or clustered.\n",
    "\n",
    "Examples:\n",
    " - Email spam classification\n",
    " - Prostate cancer regression prediction of log prostate specific antigen\n",
    " - Handwritten digit recognition\n",
    " - DNA unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning we use the term `regression` when the output is quantitative and `classification` when the output is qualitative (a.k.a categorical/discrete). A third variable type is ordered categorical, such as small, medium and large.\n",
    "\n",
    "Given X, make a good prediction of the output Y, denoted by $\\hat{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models and least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the relationship between X and Y as a linear function of X. We include a 1 in the X vector to account for the intercept. (here X is a single vector, but in general it can be a matrix where each row is a sample and each column is a feature, in this case Y would be a vector of outcomes)\n",
    "$$\\hat{Y}  = \\hat{\\beta}_0 + \\sum_{j=1}^pX_j\\hat{\\beta}_j\\\\ = X^{T}\\hat{\\beta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model the most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n",
    "Minimising this gives the least squares estimates of the coefficients \n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nearest-Neighbor Methods\n",
    "We use the k-nearest neighbors to predict the outcome of a new sample. \n",
    "$$\\hat{Y}(x) = \\frac{1}{k}\\sum_{x_i \\in N_k(x)}y_i$$\n",
    "where $N_k(x)$ is the set of k points in the training set closest to x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Theory\n",
    "\n",
    "In general we seek function $f(X)$ predicting Y. We define the loss function $L(Y, f(X))$ which measures the cost of predicting $f(X)$ when the true value is Y. The most common loss function is the squared error loss \n",
    "$$L(Y, f(X)) = (Y - f(X))^2$$\n",
    "The value of $f(X)$ that minimizes the expected prediction error is the conditional expectation of Y given X, $f(x) = E(Y|X=x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of dimensionality\n",
    "\n",
    "Looking at the nearest neighbours method, as the number of dimensions increases, the volume of the space increases so that the nearest neighbours are no longer as close. This is the curse of dimensionality and increases the bias of the nearest neighbour method.\n",
    "\n",
    "The book shows that by leveraging structure of the problem, such as linearity, we can reduce the impact of the curse of dimensionality reducing the bias and variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Supervised Learning and Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Often we can reframe the supervised problem as a statistical model where\n",
    "$$\n",
    "Y = f(X) + \\epsilon\n",
    "$$\n",
    "where $E(\\epsilon) = 0$\n",
    "\n",
    "We want to estimate $f(X)$ using the training data. We can view this as a `learning problem` where we iteratively improve our estimate of $f(X)$ or as a `function approximation problem`.\n",
    "\n",
    "To find the optimal function we often minimize the square loss but a more general approach is to maximised the likelihood of the data given some assumed model. In the case of a linear model assuming the errors are normally distributed we can show maximising the likelihood is equivalent to minimizing the square loss.\n",
    "\n",
    "Besides a linear model or nearest neighbours other methods include:\n",
    " - `Roughness penalty methods`: These reduce model complexity by penalizing the complexity of the model in the loss function. This is also known as `regularization`.\n",
    " - `Kernel methods`, similar to nearest neighbours but with a `weighted average of the neighbours` to account for the distance.\n",
    " - `Basis functions` assume f is of the form $\\sum_{j=1}^M\\theta_jh_j(x)$ where $h_j(x)$ are functions of x. These approaches linear models, splines, single layer neural networks and radial basis functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bias variance tradeoff\n",
    "\n",
    "Many models have a smoothing parameter that controls the complexity of the model. The choice of this parameter is a tradeoff between bias and variance. A model with high bias will underfit the data, while a model with high variance will overfit the data.\n",
    "\n",
    "The expected prediction error can be decomposed into the irreducible error, the squared bias and the variance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Chapter 3 - Linear methods for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression assume $E(Y|X)$ is linear in $X_i$. We have an input vector $X^T = (X_1, X_2, ..., X_p)$ and we want to predict a real-valued output Y.\n",
    "$$\n",
    "    f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j\n",
    "$$\n",
    "We have a training set of N observations $(x_1, y_1), ..., (x_N, y_N)$ and we want to estimate the coefficients $\\beta_0, \\beta_1, ..., \\beta_p$. Each $x_i$ is a p-dimensional vector $x_i = (x_{i1}, x_{i2}, ..., x_{ip})^T$.\n",
    "The most common approach is to minimize the residual sum of squares (RSS)\n",
    "$$\n",
    "    RSS(\\beta) = \\sum_{i=1}^N(y_i - x_i^T\\beta)^2\\\\\n",
    "    = (y - X\\beta)^T(y - X\\beta)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this we can differentiate the RSS with respect to $\\beta$ and set to zero\n",
    "$$\n",
    "    \\frac{\\partial RSS}{\\partial \\beta} = -2X^T(y - X\\beta) = 0\n",
    "$$\n",
    "This gives the normal equations\n",
    "$$\n",
    "    X^TX\\beta = X^Ty\n",
    "$$\n",
    "and the least squares estimate of $\\beta$ is\n",
    "$$\n",
    "    \\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted values for the training data are\n",
    "$$\n",
    "    \\hat{y} = X\\hat{\\beta} = X(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "We denote the matrix $H = X(X^TX)^{-1}X^T$ as the `hat matrix` because it puts a `hat` on y. Geometrically this matrix projects y orthogonally onto the space spanned by the columns of X.\n",
    "\n",
    "$$\n",
    "    var(\\hat{\\beta}) = var((X^TX)^{-1}X^Ty) = (X^TX)^{-1}X^Tvar(y)((X^TX)^{-1}X^T)^T = \\\\\n",
    "     = (X^TX)^{-1}X^Tvar(y)X(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}X^TX(X^TX)^{-1} = \\sigma^2(X^TX)^{-1}\n",
    "$$\n",
    "\n",
    "Typically we don't know $\\sigma^2$ so we estimate it with the residual sum of squares\n",
    "$$\n",
    "    \\hat{\\sigma}^2 = \\frac{1}{N-p-1}\\sum_{i=1}^N(y_i - x_i^T\\hat{\\beta})^2\n",
    "$$\n",
    "\n",
    "Assuming the errors are normal and the model is correct then\n",
    "$$\n",
    "\\hat{\\beta} \\sim N(\\beta, \\sigma^2(X^TX)^{-1})\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\frac{(N-p-1)\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{N-p-1}\n",
    "$$\n",
    "We can use this to construct confidence intervals for the coefficients and to test hypotheses about the coefficients. We can calculate the $z_j$ values which follow a t-distribution with N-p-1 degrees of freedom and allow us to test the null hypothesis $\\beta_j = 0$.\n",
    "$$\n",
    "z_j = \\frac{\\hat{\\beta}_j}{\\sqrt{\\hat{\\sigma}^2((X^TX)^{-1})_{jj}}}\n",
    "$$\n",
    "Note the standard error of $\\hat{\\beta}_j$ is $\\sqrt{\\hat{\\sigma}^2((X^TX)^{-1})_{jj}}$\n",
    "\n",
    "#### gauss markov theorem\n",
    "The least squares estimates are the best linear unbiased estimates (BLUE) of the coefficients. This means they have the smallest variance of all linear unbiased estimates. A linear unbiased estimate is one that is unbiased and is a linear function of the data. By linear we mean of the form $c^Ty$ where c depends on the data but not on the parameter being estimated. Unbiased means $E(c^Ty) = c^TE(y) = c^TX\\beta$.\n",
    "\n",
    "The Gram-Schmidt procedure can be used to orthogonalize the columns of X. This can be useful when the columns are highly correlated. The multiple regression coefficient $\\beta_j$ represents the marginal effect of $X_j$ on Y after adjusting for all other variables in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statisitics-PieJMifL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
